{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "afr = pd.read_csv(r\"../share/train.csv\")\n",
    "afr_test = pd.read_csv(r\"../share/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer, Imputer, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#from fuzzywuzzy import fuzz\n",
    "#from fuzzywuzzy import process\n",
    "\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "        \n",
    "    def fit(self,X, y=None):\n",
    "        self.column_names = self.attribute_names\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        return X[self.attribute_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandleCategoricalNulls(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,substitute='unknown',convert_all_to_small=True):\n",
    "        '''\n",
    "           cols must be a list\n",
    "        '''\n",
    "        self.substitute = substitute\n",
    "        self.convert_all_to_small = convert_all_to_small\n",
    "        self.column_names = []\n",
    "    def get_alpha_numeric(self,x):\n",
    "        temp_l = list()\n",
    "        for i in list(x):\n",
    "            temp = re.sub(r'[^a-zA-Z0-9]',' ', i)\n",
    "            temp = re.sub(' +',\" \",temp)\n",
    "            temp_l.append(temp)\n",
    "        return temp_l\n",
    "    def fit(self,X, y=None):\n",
    "        HandleCategorical_columns = list(X.columns) \n",
    "        self.column_names = list(X.columns) \n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        X=X.fillna(self.substitute)\n",
    "        if self.convert_all_to_small:\n",
    "            X = X.apply(lambda x: x.astype(str).str.lower())\n",
    "            #Include only alpha-numeric chars\n",
    "            #Remove any contiguous spaces    \n",
    "            for i in X.columns:\n",
    "                X[i] = self.get_alpha_numeric(X[i])\n",
    "        return X        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreqBasedCategoricalBinning( BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,buckets=20,apply=True):\n",
    "        '''\n",
    "        buckets - Desired number of classes. \n",
    "        you can get less than or equal to the desired buckets,\n",
    "        if the data is heavily skewed or if we have missing ranges\n",
    "        '''\n",
    "        #Calculate the number of bins\n",
    "        try:\n",
    "            self.bin_size=np.floor(100.0/(buckets-1))\n",
    "        except:\n",
    "            self.bin_size=5.0\n",
    "        self.freq_dict={}\n",
    "        self.column_names = []\n",
    "        self.apply = apply\n",
    "    def get_freq(self,df):\n",
    "        #Get the frequency of each level in col \n",
    "        for col in df.columns:\n",
    "            total_count=df.groupby([col]).size().reset_index()\n",
    "            total_count.columns=[col,col+'_freq_bin']\n",
    "            total_count.index = list(total_count[col])\n",
    "            total_count = total_count.drop([col],axis=1)        \n",
    "            #Save the result to a dictionary\n",
    "            self.freq_dict[col] = \\\n",
    "            np.ceil(total_count.iloc[:,[0]]/np.max(total_count.iloc[:,0])*100)\n",
    "    \n",
    "    def join(self,X,key,value):\n",
    "        #add a column called 'sorted' to save the order.\n",
    "        X['sorted'] = np.arange(len(X))\n",
    "        X.index = list(X[key])\n",
    "        X.drop([key],axis=1,inplace=True)\n",
    "        X=X.join(value,how='left')\n",
    "        #Left join may result in NaN, \n",
    "        #if we have unseen levels in the variable\n",
    "        #We will pad such values with median\n",
    "        temp_val = np.median(value.iloc[:,[0]])\n",
    "        X[[value.columns[0]]] = X[[value.columns[0]]].fillna(temp_val)\n",
    "        \n",
    "        #Bin the data\n",
    "        X[[value.columns[0]]] = np.ceil(X[[value.columns[0]]]/self.bin_size)\n",
    "        \n",
    "        #Convert the bins to int and later to str\n",
    "        X[[value.columns[0]]] = X[[value.columns[0]]].astype('int') \n",
    "        #X[[value.columns[0]]] = 'class_'+X[[value.columns[0]]].astype('str')\n",
    "        X[[value.columns[0]]] = X[[value.columns[0]]].astype('str')\n",
    "        #Reset and drop the first (new) column resulted in reset_index()\n",
    "        X=X.reset_index()\n",
    "        X.drop(X.columns[0],axis=1,inplace=True)\n",
    "        #Restore the order:\n",
    "        X.sort_values(['sorted'],inplace=True)\n",
    "        X.drop(['sorted'],axis=1,inplace=True)\n",
    "        return X\n",
    "        \n",
    "    def fit(self,X, y=None):\n",
    "        if self.apply:\n",
    "            self.get_freq(X)\n",
    "            #FreqBasedCategoricalBinning = []\n",
    "            #Do not set the column names here, to make the logic simple.\n",
    "            self.column_names = []\n",
    "            return self\n",
    "        else:\n",
    "            self.column_names = []\n",
    "            return self      \n",
    "       \n",
    "    def transform(self,X,y=None):\n",
    "        if self.apply:\n",
    "            X = X.copy()\n",
    "            for key, value in self.freq_dict.items():\n",
    "                X = self.join(X,key,value)\n",
    "            global FreqBasedCategoricalBinning_cols\n",
    "            #Set the column names here\n",
    "            self.column_names = list(X.columns)\n",
    "            return X    \n",
    "        else:\n",
    "            self.column_names = list(X.columns)\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RespBasedCategoricalBinning( BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,buckets=20,apply=True):\n",
    "        '''\n",
    "        buckets - Desired number of classes. \n",
    "        you can get less than or equal to the desired buckets,\n",
    "        if the data is heavily skewed or if we have missing ranges\n",
    "        '''\n",
    "        ##Determine the bin size\n",
    "        try:\n",
    "            self.bin_size=np.floor(100.0/(buckets-1))\n",
    "        except:\n",
    "            self.bin_size= 5.0\n",
    "        #Declare a dictionary, which will save the details\n",
    "        #in the fit() function\n",
    "        self.freq_dict={}\n",
    "        self.column_names = []\n",
    "        self.apply = apply\n",
    "        \n",
    "    ##This function will be called by fit() function.    \n",
    "    def get_probs(self,X,y):\n",
    "        #Get the columns of the X data frame\n",
    "        X_columns = list(X.columns)\n",
    "        #Combine X and y into a single data frame\n",
    "        df = X\n",
    "        \n",
    "        #_target_variable is the target column, which is a categorical column\n",
    "        df['_target_variable'] = list(y)\n",
    "        \n",
    "        #For each column in the X data frame, perform the following:\n",
    "        for col in X_columns:\n",
    "        \n",
    "            #Create a data frame with counts, group by the values of the column col\n",
    "            total_count=df.groupby([col]).size().reset_index()\n",
    "            \n",
    "            #Assign the column names to the data frame with group counts\n",
    "            total_count.columns=[col,'total']\n",
    "            \n",
    "            #Make the col as the index.\n",
    "            #Pandas join is not behaving well if we are joining on a column.\n",
    "            #So I am assigning the index using the to be joined col values\n",
    "            total_count.index = list(total_count[col])\n",
    "            \n",
    "            #Drop the col as its values are in the index already.\n",
    "            total_count = total_count.drop([col],axis=1)        \n",
    "            \n",
    "            #Now create another data frame, that contains the counts of \n",
    "            #values on col, group by target variable\n",
    "            total_grp_count = df.groupby([col,'_target_variable']).size().reset_index()\n",
    "            \n",
    "            total_grp_count.columns=[col,'_target_variable','status_total']\n",
    "            total_grp_count.index = total_grp_count[col]\n",
    "            total_grp_count = total_grp_count.drop([col],axis=1)\n",
    "            \n",
    "            #Join the two data frames: total_count and total_grp_count\n",
    "            #We have to use inner join, as we are certain that there will be NO mis-matches\n",
    "            joined_df = total_count.join(total_grp_count,how='inner')\n",
    "            joined_df = joined_df.reset_index()\n",
    "            \n",
    "            #Make sure that we have proper column names, after reset_index\n",
    "            columns = list(joined_df.columns)\n",
    "            columns[0] = col\n",
    "            joined_df.columns = columns\n",
    "            #print(joined_df)\n",
    "            \n",
    "            #Calculating the proportions\n",
    "            joined_df['proportion'] = joined_df['status_total']/joined_df['total']\n",
    "            \n",
    "            #Pivot the table\n",
    "            joined_df = pd.pivot_table(joined_df,values='proportion',\\\n",
    "                                       columns='_target_variable',index=col)\n",
    "            #DO NOT reset the index on the pivot table.\n",
    "            #As the pivoted table will have col values as the index, \n",
    "            #and this will be useful later in transform()\n",
    "            \n",
    "            #Fill the NaN values with 0\n",
    "            joined_df = joined_df.fillna(0)\n",
    "            for i in joined_df.columns:\n",
    "                joined_df[i] = joined_df[i]/np.max(joined_df[i])\n",
    "            #Rename the column names, by appending with col name. \n",
    "            #This will make sure that we do not have any duplicate columns            \n",
    "            joined_df.columns = [col + '-'+str(i).replace(\" \", \"-\") for i in joined_df.columns]\n",
    "            #Save the col and pivoted table in a dictionary.\n",
    "            #This dictionary will be used in transform() logic.\n",
    "            self.freq_dict[col] = joined_df    \n",
    "        #You have to drop the _target_variable, so that we revert back the changes to X\n",
    "        df.drop([\"_target_variable\"],axis=1,inplace=True)\n",
    "    #Define a function to help with the custom join            \n",
    "    def join(self,X,key,value):\n",
    "        '''\n",
    "           X will be a data frame, on which we have to apply the transform()\n",
    "           key will be a key in the self.freq_dict\n",
    "           value will be the pivoted table corresponding to the key\n",
    "        '''\n",
    "        ##Create a column called 'sorted'  X. This will help\n",
    "        ##us to restore the order of X later (or else there might be \n",
    "        ##change that the order of X might be disturbed)\n",
    "        X['sorted'] = np.arange(len(X))\n",
    "        #Make sure that X is indexed on the key column\n",
    "        try:\n",
    "            X.index = list(X[key])\n",
    "            X.drop([key],axis=1,inplace=True)\n",
    "            X=X.join(value,how='left')\n",
    "        except:\n",
    "            print(\"EXCEPTION/EROR: The input data frame does not have \"+key+\" column\")\n",
    "            print(\"Terminating the program\")\n",
    "         \n",
    "        #Make sure that you fill the  NaN values with median\n",
    "        #This is needed, to gracefully add unseen values in the categorical variables\n",
    "        \n",
    "        \n",
    "        for i in value.columns:\n",
    "            temp_val = np.median(value[i])\n",
    "            X[i] = X[i].fillna(temp_val)\n",
    "            X[i] = np.ceil(X[i]*100/self.bin_size)\n",
    "            X[i] = X[i].astype('int') \n",
    "            X[i] = X[i].astype('str')\n",
    "            \n",
    "        X=X.reset_index()\n",
    "        #Drop the first column, which is obtained by reset_index, as it is not needed\n",
    "        X.drop(X.columns[0],axis=1,inplace=True)\n",
    "        X.sort_values(['sorted'],inplace=True)\n",
    "        X.drop(['sorted'],axis=1,inplace=True)\n",
    "        return X\n",
    "    #fit() will just build the self.freq_dict         \n",
    "    #In the following func def, the y parameter is NOT optional\n",
    "    def fit(self,X, y):\n",
    "        if self.apply:\n",
    "            self.get_probs(X,y)\n",
    "            #To make the logic simple, we will set the column names in transform()\n",
    "            self.column_names = []\n",
    "            return self \n",
    "        else:\n",
    "            self.column_names = []\n",
    "            return self \n",
    "                 \n",
    "    #transform() will iterate over the dictionary keys,\n",
    "    #and build the transformation.\n",
    "    #As we are using the self.freq_dict items,\n",
    "    #even though the input data frame supplied to transform has \n",
    "    #extra values, we will not get any errors, and such columns will remain \n",
    "    #undisturbed. \n",
    "    def transform(self,X,y=None):\n",
    "        if self.apply:\n",
    "            X = X.copy()\n",
    "            for key, value in self.freq_dict.items():\n",
    "                X = self.join(X,key,value)\n",
    "            self.column_names = list(X.columns) \n",
    "            return X\n",
    "        else:\n",
    "            self.column_names = list(X.columns) \n",
    "            return X    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatMultiLabelTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,apply=True): \n",
    "        self.column_names = [] \n",
    "        self.apply = apply\n",
    "        self.binarizers={}\n",
    "    \n",
    "    def check_input_obj(self,X,location):\n",
    "        ##Check if input object is a pandas df, else raise exception\n",
    "        try:\n",
    "            if not isinstance(X,pd.DataFrame):\n",
    "                raise ValueError\n",
    "        except:\n",
    "            print(\"**EXCEPTION/ERROR**: In \"+ location + \\\n",
    "                  \" function of \"+self.__name__+ \". Input must be a Pandas dataframe\")\n",
    "            exit(10)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.column_names = []\n",
    "        if self.apply:         \n",
    "            ##Check if input object is a pandas df, else raise exception\n",
    "            self.check_input_obj(X,'fit()')\n",
    "            ##Create an empty dict, \n",
    "            ##which will be updated with LabelBinarizer for each column        \n",
    "            self.binarizers={}\n",
    "\n",
    "            for col in X.columns:\n",
    "                uniq_elements = list(set(X[col]))\n",
    "                #print(uniq_elements)\n",
    "                if len(uniq_elements) == 2:\n",
    "                   ##Add a dummy class\n",
    "                   #We have to name this class in a \n",
    "                   #weird fashion,so that no data has this class\n",
    "                   uniq_elements.append('d#u/m*m-y+class_991-+xya')\n",
    "                lb = LabelBinarizer()\n",
    "                self.binarizers[col] = lb.fit(uniq_elements)\n",
    "                #print(X)\n",
    "                #self.column_names.append([str(col) + \"_\" + str(j) \\ \n",
    "                #for j in list(lb.classes_) if j != 'd#u/m*m-y+class_991-+xya'])\n",
    "                self.column_names = self.column_names + \\\n",
    "                     [str(col) + \"_\" + str(j) \\\n",
    "                       for j in list(lb.classes_) \\\n",
    "                         if j != 'd#u/m*m-y+class_991-+xya']\n",
    "            #print(\"in transform\")\n",
    "            #print(\"len of self.binarizers\",len(self.binarizers))\n",
    "            #print(\"len of self.column_names\",len(self.column_names))\n",
    "            return self\n",
    "        else:\n",
    "            return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "         #print(\"in transform\")\n",
    "         #print(\"len of self.binarizers\",len(self.binarizers))\n",
    "         #print(self.apply)\n",
    "        if self.apply:\n",
    "            self.check_input_obj(X,'transform()')\n",
    "            #X_transform = np.empty()\n",
    "            temp_transformed_data = []\n",
    "            transformed_column_names = []\n",
    "            for key, value in self.binarizers.items():\n",
    "                #print(\"key=\",key)\n",
    "                #print(\"value\",value)\n",
    "                #print(\"len of temp_transformed_data\",len(temp_transformed_data))\n",
    "                try:\n",
    "                    temp_transformed_data.append(value.transform(X[key]))\n",
    "                    transformed_column_names = \\\n",
    "                   transformed_column_names + \\\n",
    "                   [str(key) + \"_\" + str(j) \\\n",
    "                    for j in list(value.classes_)]\n",
    "                except:\n",
    "                    continue                \n",
    "            return pd.DataFrame(np.concatenate(temp_transformed_data, axis=1),\\\n",
    "                                columns=transformed_column_names)[self.column_names]\n",
    "            #transformed_column_names = self.column_names + [str(col) + \"_\" + \\\n",
    "            #str(j) for j in list(lb.classes_) if j != 'd#u/m*m-y+class_991-+xya']\n",
    "            #transformed_X.columns = self.columns        \n",
    "        else:\n",
    "            self.column_names = list(X.columns)\n",
    "            return X            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmountTSHTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,method='custom'): \n",
    "        self.column_names = [] \n",
    "        self.method = method\n",
    "    \n",
    "    def check_input_obj(self,X,location):\n",
    "        ##Check if input object is a pandas df, else raise exception\n",
    "        try:\n",
    "            if not isinstance(X,pd.DataFrame):\n",
    "                raise ValueError\n",
    "        except:\n",
    "            print(\"**EXCEPTION/ERROR**: In \"+ location + \\\n",
    "                  \" function of \"+self.__name__+ \\\n",
    "                  \". Input must be a Pandas dataframe\")\n",
    "            exit(10)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.column_names = ['amount_tsh']\n",
    "        if self.method == 'custom':\n",
    "            X['amount_tsh'] = X['amount_tsh'].astype(float)\n",
    "            self.check_input_obj(X,\"fit()\") \n",
    "            #Make sure that you have all the required columns:\n",
    "            if len(set(X.columns) - set(['amount_tsh','source_class', \\\n",
    "                                         'basin', 'waterpoint_type_group'])) == 0:\n",
    "                \n",
    "                #Get the required dictionaries...\n",
    "                amount_tsh_df = X[X['amount_tsh'] != 0].groupby(['source_class', \\\n",
    "                                                                 'basin', \\\n",
    "                                                                 'waterpoint_type_group'])\\\n",
    "                                                                  ['amount_tsh'].median()\n",
    "                self.amount_tsh_dict_all_level = dict(amount_tsh_df)\n",
    "                amount_tsh_df = X[X['amount_tsh'] != 0].\\\n",
    "                                groupby(['waterpoint_type_group'])\\\n",
    "                                ['amount_tsh'].median()\n",
    "                self.amount_tsh_dict_wp = dict(amount_tsh_df)\n",
    "            else:\n",
    "                raise ValueError(\"Check the supplied columns. Must supply 'source_class', \\\n",
    "                                 'basin', 'waterpoint_type_group', 'amount_tsh' only\")\n",
    "                exit(10)\n",
    "            self.column_names = ['amount_tsh']\n",
    "            return self\n",
    "        if self.method == 'median':\n",
    "            X['amount_tsh'] = X['amount_tsh'].astype(float)\n",
    "            self.median = np.median(list(X[X['amount_tsh'] != 0]['amount_tsh']))\n",
    "            if math.isnan(self.median):\n",
    "                self.median = 0\n",
    "            self.column_names = ['amount_tsh']\n",
    "            return self\n",
    "        if self.method == 'mean':\n",
    "            X['amount_tsh'] = X['amount_tsh'].astype(float)\n",
    "            self.mean = np.mean(list(X[X['amount_tsh'] > 0]['amount_tsh']))\n",
    "            if math.isnan(self.mean):\n",
    "                self.mean = 0\n",
    "            self.column_names = ['amount_tsh']\n",
    "            return self\n",
    "        if self.method == 'ignore':\n",
    "            self.column_names = ['amount_tsh']\n",
    "            return self   \n",
    "\n",
    "    def transform(self,X):\n",
    "        if self.method == 'custom':\n",
    "            X['amount_tsh'] = X['amount_tsh'].astype(float)\n",
    "            self.check_input_obj(X,\"transform()\")\n",
    "            transformed_amount_tsh = []\n",
    "            for i, j, k, l in list(zip(X['amount_tsh'].\\\n",
    "                                       fillna(0),X['source_class'], \\\n",
    "                                       X['basin'], X['waterpoint_type_group'])):\n",
    "                if i == 0:\n",
    "                    try:\n",
    "                        transformed_amount_tsh.append(self.amount_tsh_dict_all_level[(j,k,l)])\n",
    "\n",
    "                    except:\n",
    "                        try:\n",
    "                            transformed_amount_tsh.append(self.amount_tsh_dict_wp[l])\n",
    "                        except:\n",
    "                                transformed_amount_tsh.append(i)\n",
    "                                continue\n",
    "                else:\n",
    "                        transformed_amount_tsh.append(i)\n",
    "            X['amount_tsh'] = transformed_amount_tsh\n",
    "            return X[['amount_tsh']]\n",
    "        if self.method == 'median':\n",
    "            X['amount_tsh'] = X['amount_tsh'].astype(float)\n",
    "            X['amount_tsh'] = X['amount_tsh'].fillna(0)\n",
    "            amount_tsh = np.array(list(X['amount_tsh']))\n",
    "            amount_tsh[amount_tsh == 0] = self.median\n",
    "            X['amount_tsh']  = amount_tsh\n",
    "            return X[['amount_tsh']]\n",
    "        if self.method == 'mean':\n",
    "            X['amount_tsh'] = X['amount_tsh'].astype(float)\n",
    "            X['amount_tsh'] = X['amount_tsh'].fillna(0)\n",
    "            amount_tsh = np.array(list(X['amount_tsh']))\n",
    "            amount_tsh[amount_tsh == 0] = self.mean\n",
    "            X['amount_tsh']  = amount_tsh\n",
    "            return X[['amount_tsh']]\n",
    "        if self.method == 'ignore':\n",
    "            X['amount_tsh'] = X['amount_tsh'].astype(float)\n",
    "            X['amount_tsh'] = X['amount_tsh'].fillna(0)          \n",
    "            return X[['amount_tsh']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPSHeightTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,init_radius=0.1,increment_radius=0.3,method = 'custom'): \n",
    "        self.column_names = [] \n",
    "        self.init_radius = init_radius\n",
    "        self.increment_radius = increment_radius\n",
    "        self.method = method\n",
    "\n",
    "    \n",
    "    def get_subset_records(self, latitude,longitude,df,radius):\n",
    "        latitude_from = latitude - radius\n",
    "        latitude_to = latitude + radius\n",
    "        longitude_from = longitude - radius\n",
    "        longitude_to = longitude + radius\n",
    "        \n",
    "        df_temp = df[(df['latitude'] >= latitude_from) & (df['latitude'] <= latitude_to) & \\\n",
    "                  (df['longitude'] >= longitude_from) & (df['longitude'] <= longitude_to)]\n",
    "        return df_temp\n",
    "       \n",
    "    \n",
    "    def check_input_obj(self,X,location):\n",
    "        ##Check if input object is a pandas df, else raise exception\n",
    "        try:\n",
    "            if not isinstance(X,pd.DataFrame):\n",
    "                raise ValueError\n",
    "        except:\n",
    "            print(\"**EXCEPTION/ERROR**: In \"+ \\\n",
    "                  location + \" function of \"+\\\n",
    "                  self.__name__+ \\\n",
    "                  \". Input must be a Pandas dataframe\")\n",
    "            exit(10)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if self.method == 'custom':\n",
    "            X['gps_height'] = X['gps_height'].astype(float)\n",
    "            X['latitude'] = X['latitude'].astype(float)\n",
    "            X['longitude'] = X['longitude'].astype(float)\n",
    "            self.df = X[X['gps_height'] != 0]\n",
    "            self.column_names = ['gps_height']\n",
    "            return self\n",
    "        if self.method == 'median':\n",
    "            X['gps_height'] = X['gps_height'].astype(float)\n",
    "            #X['gps_height'] = X['gps_height'].fillna(0)\n",
    "            self.median = np.median(list(X[X['gps_height'] != 0]['gps_height']))\n",
    "\n",
    "            if math.isnan(self.median):\n",
    "                self.median = 0\n",
    "            self.column_names = ['gps_height']\n",
    "            return self\n",
    "        \n",
    "        if self.method == 'mean':\n",
    "            X['gps_height'] = X['gps_height'].astype(float)\n",
    "            #X['gps_height'] = X['gps_height'].fillna(0)\n",
    "            self.mean = np.mean(list(X[X['gps_height'] != 0]['gps_height']))\n",
    "            if math.isnan(self.mean):\n",
    "                self.mean = 0\n",
    "            self.column_names = ['gps_height']\n",
    "            return self\n",
    "        if self.method == 'ignore':\n",
    "            self.column_names = ['gps_height']\n",
    "            return self      \n",
    "        \n",
    "    def transform(self,X):\n",
    "        if self.method == 'custom':\n",
    "            X['gps_height'] = X['gps_height'].astype(float)\n",
    "            X['latitude'] = X['latitude'].astype(float)\n",
    "            X['longitude'] = X['longitude'].astype(float)\n",
    "            \n",
    "            gps_height_transformed = []\n",
    "            for latitude, longitude, gps_height in \\\n",
    "                zip(X['latitude'],X['longitude'],X['gps_height']):\n",
    "                radius = self.init_radius\n",
    "                if gps_height == 0:\n",
    "                    gps_height_temp = gps_height\n",
    "                    while gps_height_temp == 0 and radius <= 2:\n",
    "                        df_temp = self.get_subset_records\\\n",
    "                                  (latitude,longitude,self.df,radius)\n",
    "                        \n",
    "                        gps_height_temp = np.mean(df_temp[df_temp['gps_height']!=0]\\\n",
    "                                                  ['gps_height'])\n",
    "                        if math.isnan(gps_height_temp):\n",
    "                            gps_height_temp = 0 \n",
    "                        radius = self.increment_radius + radius\n",
    "                else:\n",
    "                    gps_height_temp =gps_height\n",
    "                gps_height_transformed.append(gps_height_temp)\n",
    "            X['gps_height'] = gps_height_transformed\n",
    "            self.column_names = ['gps_height']\n",
    "            #self.column_names = list(X.columns)\n",
    "            #return X[['latitude','longitude','gps_height']]\n",
    "            return X[['gps_height']]\n",
    "        if self.method == 'median':\n",
    "            X['gps_height'] = X['gps_height'].astype(float)\n",
    "            X['gps_height'] = X['gps_height'].fillna(0)\n",
    "            gps_height = np.array(list(X['gps_height']))\n",
    "            gps_height[gps_height == 0] = self.median\n",
    "            self.column_names = ['gps_height']\n",
    "            #self.column_names = list(X.columns)\n",
    "            #return X[['latitude','longitude','gps_height']]\n",
    "            X['gps_height'] = gps_height\n",
    "            return X[['gps_height']]\n",
    "        if self.method == 'mean':\n",
    "            X['gps_height'] = X['gps_height'].astype(float)\n",
    "            X['gps_height'] = X['gps_height'].fillna(0)\n",
    "            gps_height = np.array(list(X['gps_height']))\n",
    "            gps_height[gps_height == 0] = self.mean\n",
    "            self.column_names = ['gps_height']\n",
    "            #self.column_names = list(X.columns)\n",
    "            #return X[['latitude','longitude','gps_height']]\n",
    "            X['gps_height'] = gps_height\n",
    "            return X[['gps_height']]\n",
    "        if self.method == 'ignore':\n",
    "            self.column_names = ['gps_height']\n",
    "            X['gps_height'] = X['gps_height'].astype(float)\n",
    "            X['gps_height'] = X['gps_height'].fillna(0)\n",
    "            return X[['gps_height']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopulationTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,init_radius=0.1,increment_radius=0.3,method = 'custom'): \n",
    "        self.column_names = [] \n",
    "        self.init_radius = init_radius\n",
    "        self.increment_radius = increment_radius\n",
    "        self.method = method\n",
    "\n",
    "    \n",
    "    def get_subset_records(self, latitude,longitude,df,radius):\n",
    "        latitude_from = latitude - radius\n",
    "        latitude_to = latitude + radius\n",
    "        longitude_from = longitude - radius\n",
    "        longitude_to = longitude + radius\n",
    "        \n",
    "        df_temp = df[(df['latitude'] >= \\\n",
    "                      latitude_from) & (df['latitude'] <= latitude_to) & \\\n",
    "                  (df['longitude'] >= longitude_from) & \\\n",
    "                  (df['longitude'] <= longitude_to)]\n",
    "        return df_temp\n",
    "       \n",
    "    \n",
    "    def check_input_obj(self,X,location):\n",
    "        ##Check if input object is a pandas df, else raise exception\n",
    "        try:\n",
    "            if not isinstance(X,pd.DataFrame):\n",
    "               raise ValueError\n",
    "        except:\n",
    "            print(\"**EXCEPTION/ERROR**: In \"+ location + \\\n",
    "                  \" function of \"+self.__name__+ \". Input must be a Pandas dataframe\")\n",
    "            exit(10)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if self.method == 'custom':\n",
    "            X['latitude'] = X['latitude'].astype(float)\n",
    "            X['longitude'] = X['longitude'].astype(float)\n",
    "            X['population'] = X['population'].astype(float)\n",
    "            self.df = X[X['population'] > 1]\n",
    "            self.column_names = ['population']\n",
    "            return self\n",
    "        if self.method == 'median':\n",
    "            X['population'] = X['population'].astype(float)\n",
    "            #X['gps_height'] = X['gps_height'].fillna(0)\n",
    "            self.median = np.median(list(X[X['population'] != 0]['population']))\n",
    "            if math.isnan(self.median):\n",
    "                self.median = 0\n",
    "            self.column_names = ['population']\n",
    "            return self\n",
    "        if self.method == 'mean':\n",
    "            X['population'] = X['population'].astype(float)\n",
    "            #X['gps_height'] = X['gps_height'].fillna(0)\n",
    "            self.mean = np.mean(list(X[X['population'] != 0]['population']))\n",
    "            if math.isnan(self.mean):\n",
    "                self.mean = 0\n",
    "            self.column_names = ['population']   \n",
    "            return self\n",
    "        if self.method == 'ignore':\n",
    "            self.column_names = ['population']\n",
    "            return self      \n",
    "       \n",
    "    def transform(self,X):\n",
    "        self.column_names = ['population']\n",
    "        if self.method == 'custom':      \n",
    "            X['latitude'] = X['latitude'].astype(float)\n",
    "            X['longitude'] = X['longitude'].astype(float)\n",
    "            X['population'] = X['population'].astype(float)\n",
    "        \n",
    "            population_transformed = []\n",
    "            for latitude, longitude, population in \\\n",
    "                zip(X['latitude'],X['longitude'],X['population']):\n",
    "                radius = self.init_radius\n",
    "                if population <= 1:\n",
    "                    population_temp = population\n",
    "                    while population_temp <= 1 and radius <= 2:\n",
    "                        df_temp = self.get_subset_records\\\n",
    "                                  (latitude,longitude,self.df,radius)\n",
    "                        \n",
    "                        population_temp = np.mean(df_temp['population'])\n",
    "                        if math.isnan(population_temp):\n",
    "                            population_temp = population \n",
    "                        radius = self.increment_radius + radius\n",
    "                else:\n",
    "                    population_temp =population\n",
    "                population_transformed.append(population_temp)\n",
    "            X['population'] = population_transformed\n",
    "            #self.column_names = ['population']\n",
    "            #self.column_names = list(X.columns)\n",
    "            self.column_names = ['population']\n",
    "            return X[['population']]\n",
    "\n",
    "        if self.method == 'median':      \n",
    "                X['population'] = X['population'].astype(float)\n",
    "                X['population'] = X['population'].fillna(0)\n",
    "                population = np.array(list(X['population']))\n",
    "                population[population == 0] = self.median\n",
    "                self.column_names = ['population']\n",
    "                #self.column_names = list(X.columns)\n",
    "                #return X[['latitude','longitude','gps_height']]\n",
    "                X['population'] = population\n",
    "                return X[['population']]\n",
    "\n",
    "        if self.method == 'mean':\n",
    "                X['population'] = X['population'].astype(float)\n",
    "                X['population'] = X['population'].fillna(0)\n",
    "                population = np.array(list(X['population']))\n",
    "                population[population == 0] = self.mean\n",
    "                self.column_names = ['population']\n",
    "                #self.column_names = list(X.columns)\n",
    "                #return X[['latitude','longitude','gps_height']]\n",
    "                X['population'] = population\n",
    "                return X[['population']]\n",
    "        \n",
    "        if self.method == 'ignore':      \n",
    "                X['population'] = X['population'].astype(float)\n",
    "                X['population'] = X['population'].fillna(0)\n",
    "                return X[['population']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YearTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,method = 'custom'): \n",
    "        self.column_names = [] \n",
    "        #self.init_radius = init_radius\n",
    "        #self.increment_radius = increment_radius\n",
    "        self.method = method\n",
    "        pass ##Nothing else to do\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        X['construction_year'] = X['construction_year'].astype(float)\n",
    "        if self.method == 'custom':\n",
    "            year_recorded = X[X['construction_year'] > 0]\\\n",
    "                            ['date_recorded'].\\\n",
    "                            apply(lambda x: int(x.split(\"-\")[0]))\n",
    "            year_constructed = X[X['construction_year'] > 0]['construction_year']\n",
    "            self.median_age = np.median(year_recorded - year_constructed)\n",
    "            self.column_names = ['age']\n",
    "            return self\n",
    "        if self.method == 'median':\n",
    "            X['construction_year'] = X['construction_year'].astype(float)\n",
    "            #X['gps_height'] = X['gps_height'].fillna(0)\n",
    "            self.median = \\\n",
    "                          np.median(list(X[X['construction_year'] != 0]['construction_year']))\n",
    "            if math.isnan(self.median):\n",
    "                self.median = 0\n",
    "            self.column_names = ['construction_year']\n",
    "            return self\n",
    "        if self.method == 'mean':\n",
    "            X['construction_year'] = X['construction_year'].astype(float)\n",
    "            #X['gps_height'] = X['gps_height'].fillna(0)\n",
    "            self.mean = np.mean(list(X[X['construction_year'] != 0]['construction_year']))\n",
    "            if math.isnan(self.mean):\n",
    "                self.mean = 0\n",
    "            self.column_names = ['construction_year']\n",
    "            return self\n",
    "\n",
    "        if self.method == 'ignore':\n",
    "            self.column_names = ['construction_year']\n",
    "            return self\n",
    "          \n",
    "    def transform(self,X):\n",
    "        if self.method == 'custom':\n",
    "            year_recorded = list(X['date_recorded'].apply(lambda x: int(x.split(\"-\")[0])))\n",
    "            year_constructed = list(X['construction_year'])\n",
    "            age = []\n",
    "            for i,j in enumerate(year_constructed):\n",
    "                if j == 0:\n",
    "                    age.append(self.median_age)\n",
    "                else:\n",
    "                    temp_age = year_recorded[i] - year_constructed[i]\n",
    "                    if temp_age < 0:\n",
    "                        temp_age = self.median_age\n",
    "                    age.append(temp_age)   \n",
    "            X['age'] = age\n",
    "            self.column_names = ['age']\n",
    "            #self.column_names = X.columns\n",
    "            return X[['age']]\n",
    "        if self.method == 'median':      \n",
    "                X['construction_year'] = X['construction_year'].astype(float)\n",
    "                X['construction_year'] = X['construction_year'].fillna(0)\n",
    "                construction_year = np.array(list(X['construction_year']))\n",
    "                construction_year[construction_year == 0] = self.median\n",
    "                self.column_names = ['construction_year']\n",
    "                X['construction_year'] = construction_year\n",
    "                return X[['construction_year']]\n",
    "\n",
    "        if self.method == 'mean':\n",
    "                X['construction_year'] = X['construction_year'].astype(float)\n",
    "                X['construction_year'] = X['construction_year'].fillna(0)\n",
    "                construction_year = np.array(list(X['construction_year']))\n",
    "                construction_year[construction_year == 0] = self.mean\n",
    "                self.column_names = ['construction_year']\n",
    "                X['construction_year'] = construction_year\n",
    "                return X[['construction_year']]\n",
    "        \n",
    "        if self.method == 'ignore':      \n",
    "                X['construction_year'] = X['construction_year'].astype(float)\n",
    "                X['construction_year'] = X['construction_year'].fillna(0)\n",
    "                self.column_names = ['construction_year']\n",
    "                return X[['construction_year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatitudeLongitudeProcess( BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,strategy='median'):\n",
    "        '''\n",
    "           type = 'median' is the default.\n",
    "           other values of type can be 'custom' \n",
    "        ''' \n",
    "        self.strategy = strategy\n",
    "        self.median_longitude = 0\n",
    "        self.custom_longitude = 0\n",
    "        self.median_latitude = 0\n",
    "        self.custom_latitude = 0\n",
    "        self.avg_lat_ward_dict = {}\n",
    "        self.avg_long_ward_dict = {}\n",
    "        self.avg_lat_lga_dict = {}\n",
    "        self.avg_long_lga_dict = {}\n",
    "        self.avg_lat_region_dict = {}\n",
    "        self.avg_long_region_dict = {}\n",
    "        self.avg_lat_country_dict = {}\n",
    "        self.avg_long_country_dict = {}\n",
    "        self.column_names = [] \n",
    "        \n",
    "    def get_level_means(self,X):\n",
    "        if 'ward' in X.columns:\n",
    "            #Get average of lats and longs at the ward level\n",
    "            #First delete rows that have unknown ward values:\n",
    "            df = X[~((X['ward'].isnull()) | (X['ward'] == 'unknown'))]\n",
    "            avg_lat_long_by_ward_df = df[df['longitude'] != 0]. \\\n",
    "            groupby(['ward'])['latitude','longitude'].mean().reset_index()\n",
    "            if len(avg_lat_long_by_ward_df) > 0:\n",
    "                avg_lat_long_by_ward_df.columns=['ward','avg_latitude','avg_longitude']\n",
    "                self.avg_lat_ward_dict = dict(zip(list(avg_lat_long_by_ward_df['ward']),\\\n",
    "                                                      list(avg_lat_long_by_ward_df['avg_latitude'])))\n",
    "                self.avg_long_ward_dict = dict(zip(list(avg_lat_long_by_ward_df['ward']),\\\n",
    "                                                       list(avg_lat_long_by_ward_df['avg_longitude'])))\n",
    "        if 'lga' in X.columns:        \n",
    "            #Get average of lats and longs at the lga level\n",
    "            #First delete rows that have unknown region values:\n",
    "            df = X[~((X['lga'].isnull()) | (X['lga'] == 'unknown'))]\n",
    "            avg_lat_long_by_lga_df = df[df['longitude'] != 0]. \\\n",
    "                groupby(['lga'])['latitude','longitude'].mean().reset_index()\n",
    "            if len(avg_lat_long_by_lga_df) > 0:\n",
    "                avg_lat_long_by_lga_df.columns=['lga','avg_latitude','avg_longitude']\n",
    "                self.avg_lat_lga_dict = dict(zip(list(avg_lat_long_by_lga_df['lga']),\n",
    "                                                     list(avg_lat_long_by_ward_df['avg_latitude'])))\n",
    "                self.avg_long_lga_dict = dict(zip(list(avg_lat_long_by_lga_df['lga']),\n",
    "                                                      list(avg_lat_long_by_ward_df['avg_longitude'])))\n",
    "        if 'region' in X.columns:                \n",
    "            #Get average of lats and longs at the region level\n",
    "            #First delete rows that have unknown region values:\n",
    "            df = X[~((X['region'].isnull()) | (X['region'] == 'unknown'))]\n",
    "            avg_lat_long_by_region_df = df[df['longitude'] != 0]. \\\n",
    "                groupby(['region'])['latitude','longitude'].mean().reset_index()\n",
    "            if len(avg_lat_long_by_region_df) > 0:\n",
    "                avg_lat_long_by_region_df.columns=['region','avg_latitude','avg_longitude']\n",
    "                self.avg_lat_region_dict = dict(zip(list(avg_lat_long_by_region_df['region']),\\\n",
    "                                   list(avg_lat_long_by_region_df['avg_latitude'])))\n",
    "                self.avg_long_region_dict = dict(zip(list(avg_lat_long_by_region_df['region']),\\\n",
    "                                    list(avg_lat_long_by_region_df['avg_longitude'])))\n",
    "                \n",
    "        #Get average of lats and longs at the country level\n",
    "        avg_long = np.mean(X[X['longitude'] != 0]['longitude'])\n",
    "        avg_lat = np.mean(X[X['latitude'] != 0]['latitude'])\n",
    "        self.avg_lat_country_dict['country'] = avg_lat\n",
    "        self.avg_long_country_dict['country'] = avg_long\n",
    "        \n",
    "    def fit(self,X, y=None):\n",
    "        self.column_names = ['latitude','longitude']\n",
    "        X['latitude'] = X['latitude'].astype(float)\n",
    "        X['longitude'] = X['longitude'].astype(float)\n",
    "        if self.strategy == 'custom':\n",
    "            self.get_level_means(X)\n",
    "        elif self.strategy == 'mean':\n",
    "            #Impute using mean\n",
    "            self.mean_longitude = np.mean(X[X['longitude'] != 0]['longitude'])\n",
    "            self.mean_latitude = np.mean(X[X['latitude'] != 0]['latitude'])           \n",
    "            #X.longitude = [i for i in X.longitude if np.abs(i) <= 0 self.mean_longitude else i]\n",
    "            #X.latitude  = [i for i in X.latitude if np.abs(i) <= 0 self.mean_latitude else i]\n",
    "        elif self.strategy == 'median':\n",
    "            #Impute using median\n",
    "            self.median_longitude = np.median(X[X['longitude'] != 0]['longitude'])\n",
    "            self.median_latitude = np.median(X[X['latitude'] != 0]['latitude'])           \n",
    "            #X.longitude = [i for i in X.longitude if np.abs(i) <= 0 self.median_longitude else i]\n",
    "            #X.latitude  = [i for i in X.latitude if np.abs(i) <= 0 self.median_latitude else i]\n",
    "        else:\n",
    "            print(\"Invalid strategy supplied for LatitudeLongitudeProcess.\")\n",
    "            print(\"Valid values are 'mean', 'median' or 'custom'. Terminating the program\")\n",
    "            exit(10)\n",
    "        return self\n",
    "    def make_up_lat_long(self,X):\n",
    "        #Handle the situation gracefully, if the incoming data does not have any required columns\n",
    "        try:\n",
    "            latitude_list = list(X['latitude'].fillna(0))\n",
    "        except:\n",
    "            latitude_list = list(np.zeros(len_X))\n",
    "            #continue\n",
    "        try:          \n",
    "            longitude_list = list(X['longitude'].fillna(0))\n",
    "        except:\n",
    "            longitude_list = list(np.zeros(len_X))\n",
    "            #continue\n",
    "        return latitude_list, longitude_list   \n",
    "        \n",
    "    def custom_transform(self, X):\n",
    "        len_X = len(X)\n",
    "          \n",
    "        #Declare lists to hold the transformed lat and long\n",
    "        latitude_transformed = []\n",
    "        longitude_transformed = []\n",
    "          \n",
    "        #Handle the situation gracefully, if the incoming data does not have any required columns\n",
    "        latitude_list, longitude_list =  self.make_up_lat_long(X)\n",
    "          \n",
    "        try:\n",
    "            ward_list = list(X['ward'].fillna('unknown'))\n",
    "        except:\n",
    "            ward_list = ['unknown'] * len_X\n",
    "            #continue\n",
    "              \n",
    "        try:    \n",
    "            lga_list = list(X['lga'].fillna('unknown'))\n",
    "        except:\n",
    "            lga_list = ['unknown'] * len_X\n",
    "            #continue\n",
    "              \n",
    "        try:    \n",
    "            region_list = list(X['region'].fillna('unknown'))\n",
    "        except:\n",
    "            region_list = ['unknown'] * len_X\n",
    "            #continue\n",
    "              \n",
    "        for (i, j, k, l, m) in zip(latitude_list,longitude_list, \\\n",
    "                                    ward_list,lga_list,region_list):\n",
    "            if np.round(i) == 0 or np.round(j) == 0:\n",
    "                try:\n",
    "                    latitude_transformed.append(self.avg_lat_ward_dict[k])\n",
    "                    longitude_transformed.append(self.avg_long_ward_dict[k])\n",
    "                except:\n",
    "                    try:\n",
    "                        latitude_transformed.append(self.avg_lat_lga_dict[l])\n",
    "                        longitude_transformed.append(self.avg_long_lga_dict[l])\n",
    "                        continue\n",
    "                    except:\n",
    "                        try:\n",
    "                            latitude_transformed.append(avg_lat_region_dict[m])\n",
    "                            longitude_transformed.append(avg_long_region_dict[m])\n",
    "                            continue\n",
    "                        except:   \n",
    "                            latitude_transformed.append(self.avg_lat_country_dict['country'])\n",
    "                            longitude_transformed.append(self.avg_long_country_dict['country'])\n",
    "                            continue\n",
    "            else:\n",
    "                latitude_transformed.append(i)\n",
    "                longitude_transformed.append(j)     \n",
    "        X['latitude'] = latitude_transformed\n",
    "        X['longitude'] = longitude_transformed          \n",
    "        return X\n",
    "    def transform(self,X):\n",
    "        X['latitude'] = X['latitude'].astype(float)\n",
    "        X['longitude'] = X['longitude'].astype(float)\n",
    "        self.column_names = ['latitude','longitude'] \n",
    "        if self.strategy == 'custom':\n",
    "            X = self.custom_transform(X)\n",
    "            return X[['latitude','longitude']]\n",
    "        elif self.strategy == 'mean':\n",
    "            latitude_list, longitude_list =  self.make_up_lat_long(X)\n",
    "            latitude_list = np.array(latitude_list)\n",
    "            longitude_list = np.array(longitude_list)\n",
    "            latitude_list[latitude_list == 0] = self.mean_latitude\n",
    "            longitude_list[longitude_list == 0] = self.mean_longitude\n",
    "            X['latitude'] = latitude_list\n",
    "            X['longitude'] = longitude_list\n",
    "            return X[['latitude','longitude']]\n",
    "        elif self.strategy == 'median':\n",
    "            latitude_list, longitude_list =  self.make_up_lat_long(X)\n",
    "            latitude_list = np.array(latitude_list)\n",
    "            longitude_list = np.array(longitude_list)\n",
    "            latitude_list[latitude_list == 0] = self.median_latitude\n",
    "            longitude_list[longitude_list == 0] = self.median_longitude\n",
    "            X['latitude'] = latitude_list\n",
    "            X['longitude'] = longitude_list\n",
    "            return X[['latitude','longitude']]\n",
    "             #self.column_names = list(X.columns)\n",
    "             #return X\n",
    "        else:\n",
    "            print(\"EXCEPTION: The supplied strategy\",self.strategy,\" is incorrect\")\n",
    "            exit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunderInstTransformer( BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,initial_chars=3,groups=15,apply=True):\n",
    "        self.initial_chars = initial_chars\n",
    "        self.groups = groups\n",
    "        self.apply = apply\n",
    "        self.group_dict = dict()\n",
    "    def fetch_first_n_chars(self,l):\n",
    "        temp_l = [str(j).lower()[0:self.initial_chars] for j in list(l)]\n",
    "        return pd.Series(temp_l)\n",
    "                \n",
    "    def fit(self,X, y=None):\n",
    "        if self.apply:\n",
    "            self.column_names = ['funder','installer'] \n",
    "            self.group_dict = dict()\n",
    "            for i in X.columns:\n",
    "                    temp_series = self.fetch_first_n_chars(X[i]).value_counts()\n",
    "                    temp_series.sort_values(ascending=False,inplace=True)\n",
    "                    top_groups = list(temp_series[0:self.groups].index)\n",
    "                    self.group_dict[i] = top_groups\n",
    "\n",
    "            return self\n",
    "        else:\n",
    "            return self\n",
    "        \n",
    "    def transform(self,X):\n",
    "        X = X.copy()\n",
    "        if self.apply:\n",
    "            for i in X.columns:\n",
    "                    temp_series = self.fetch_first_n_chars(X[i])\n",
    "                    temp_l = []\n",
    "                    for j in temp_series.values:\n",
    "                        try:\n",
    "                            #print(self.group_dict[i])\n",
    "                            if j in self.group_dict[i]:\n",
    "                                #print(j)\n",
    "                                temp_l.append(j)  \n",
    "                            else: \n",
    "                                temp_l.append('other')  \n",
    "                        except:\n",
    "                            continue\n",
    "                    X[i] = temp_l\n",
    "            #X['funder_installer_same'] = X['funder']==X['installer']\n",
    "            #self.column_names = ['funder','installer','funder_installer_same']\n",
    "            #return X[['funder','installer','funder_installer_same']]         \n",
    "            self.column_names = ['funder','installer']\n",
    "            return X[['funder','installer']]         \n",
    "        else:\n",
    "            #self.column_names = ['funder','installer','funder_installer_same']\n",
    "            #return X[['funder','installer','funder_installer_same']]\n",
    "            self.column_names = ['funder','installer']\n",
    "            return X[['funder','installer']]         \n",
    "         \n",
    "class Numpy2DFTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,columns): \n",
    "        self.column_names = columns\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "\n",
    "        return pd.DataFrame(X,columns=self.column_names)\n",
    "    ##############\n",
    "    ##IMPORTANT###\n",
    "    ##############\n",
    "    #https://stackoverflow.com/questions/41837261/data-not-persistent-in-scikit-learn-transformers\n",
    "    #get_params() is very important to get the data persistent between\n",
    "    #the CV evaluation. If NOT defined, then the init params are NOT set and results in \n",
    "    #CV (GridSearch) failure  \n",
    "    def get_params(self, deep=False):\n",
    "        return {'columns': self.column_names}\n",
    "\n",
    "#Scale numeric data        \n",
    "class ScaleData(BaseEstimator, TransformerMixin): \n",
    "     def __init__(self,std_scaler=True,apply=True):\n",
    "            self.std_scaler = std_scaler\n",
    "            self.apply = apply\n",
    " \n",
    "     def fit(self,X, y=None):\n",
    "        if self.apply: \n",
    "            if self.std_scaler == True: \n",
    "                self.scaler = StandardScaler() \n",
    "            else: \n",
    "                self.scaler = MinMaxScaler() \n",
    "            return self.scaler.fit(X) \n",
    "        else:\n",
    "            return X        \n",
    " \n",
    "     def transform(self,X): \n",
    "        if self.apply:\n",
    "            return self.scaler.transform(X) \n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChooseCatPipelineType(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self,freq_pipeline,resp_pipeline,method='both'):\n",
    "            self.freq_pipeline = freq_pipeline\n",
    "            self.resp_pipeline = resp_pipeline\n",
    "            self.method = method\n",
    "            self.column_names = []\n",
    "    def fit(self,X, y=None): \n",
    "        if self.method == 'resp':\n",
    "            self.resp_pipeline.fit(X,y) \n",
    "        if self.method == 'freq':\n",
    "            self.freq_pipeline.fit(X,y) \n",
    "        if self.method == 'both':\n",
    "            self.both_pipeline = FeatureUnion(transformer_list = [ \\\n",
    "                                    (\"freq_pipeline\",self.freq_pipeline), \\\n",
    "                                    (\"resp_pipeline\",self.resp_pipeline) \\\n",
    "                                                ])\n",
    "            self.both_pipeline.fit(X,y)                                                \n",
    "        return self\n",
    "\n",
    "    def transform(self,X): \n",
    "        if self.method == 'resp':\n",
    "            self.column_names = self.resp_pipeline.named_steps['CatMultiLabelTransformer'].column_names\n",
    "            return self.resp_pipeline.transform(X) \n",
    "        if self.method == 'freq':\n",
    "            self.column_names = self.freq_pipeline.named_steps['CatMultiLabelTransformer'].column_names\n",
    "            return self.freq_pipeline.transform(X) \n",
    "        if self.method == 'both':\n",
    "            self.column_names = self.freq_pipeline.named_steps['CatMultiLabelTransformer'].column_names + \\\n",
    "                              self.resp_pipeline.named_steps['CatMultiLabelTransformer'].column_names\n",
    "            return self.both_pipeline.transform(X)                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnf_test\n",
    "def error_met(y_test, y_check):\n",
    "    \n",
    "    cnf = confusion_matrix(y_test, y_check)\n",
    "    \n",
    "    fpr = cnf[1,0]/cnf[1,:].sum()\n",
    "    fnr = cnf[0,1]/cnf[0,:].sum()\n",
    "    return 0.9*fpr+0.1*fnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error_met(cnf_train)\n",
    "# cnf_test\n",
    "def error(y_test, y_check):\n",
    "    \n",
    "    cnf = confusion_matrix(y_test, y_check)\n",
    "    \n",
    "    fpr = cnf[1,0]/cnf[1,:].sum()\n",
    "    fnr = cnf[0,1]/cnf[0,:].sum()\n",
    "    return -(0.9*fpr+0.1*fnr)*accuracy_score(y_test, y_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score, make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_type = {\n",
    "'new_ids':int,\n",
    "'amount_tsh': float,\n",
    "'date_recorded': str,\n",
    "'funder': str,\n",
    "'gps_height': float,\n",
    "'installer': str,\n",
    "'longitude': float,\n",
    "'latitude': float,\n",
    "'wpt_name': str,\n",
    "'num_private': float,\n",
    "'basin': str,\n",
    "'subvillage': str,\n",
    "'region': str,\n",
    "'region_code': str,\n",
    "'district_code': str,\n",
    "'lga': str,\n",
    "'ward': str,\n",
    "'population': float,\n",
    "'public_meeting': str,\n",
    "'recorded_by': str,\n",
    "'scheme_management': str,\n",
    "'scheme_name': str,\n",
    "'permit': str,\n",
    "'construction_year': int,\n",
    "'extraction_type': str,\n",
    "'extraction_type_group': str,\n",
    "'extraction_type_class': str,\n",
    "'management': str,\n",
    "'management_group': str,\n",
    "'payment': str,\n",
    "'payment_type': str,\n",
    "'water_quality': str,\n",
    "'quality_group': str,\n",
    "'quantity': str,\n",
    "'quantity_group': str,\n",
    "'source': str,\n",
    "'source_type': str,\n",
    "'source_class': str,\n",
    "'waterpoint_type': str,\n",
    "'waterpoint_type_group': str\n",
    "}                      \n",
    "\n",
    "train_label_type = {\n",
    "'new_ids':int,\n",
    "'defective': str\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_levels_cat_columns = ['subvillage','lga','ward']#,'scheme_name']\n",
    "\n",
    "#Columns with less number of levels.\n",
    "low_levels_cat_columns = ['basin','region','district_code',\n",
    "                          'public_meeting',\n",
    "                          'scheme_management',\n",
    "                          'permit',\n",
    "                          #'extraction_type',\n",
    "                          'extraction_type_group',\n",
    "                          #'extraction_type_class',\n",
    "                          'management',\n",
    "                          #'management_group',\n",
    "                          'payment_type',\n",
    "                          'water_quality',\n",
    "                          #'quality_group',\n",
    "                          'quantity_group',\n",
    "                          'source',\n",
    "                          #'source_type','source_class',\n",
    "                          'waterpoint_type'\n",
    "                          #,'waterpoint_type_group'\n",
    "                          ]\n",
    "\n",
    "#Columns which need fuzzy matching. These columns have many levels.\n",
    "fuzzy_logic_columns = ['funder','installer']\n",
    "\n",
    "\n",
    "#Pipelines definition\n",
    "fuzz_pipeline = Pipeline([ \\\n",
    "                         ('selector',DataFrameSelector(fuzzy_logic_columns)), \\\n",
    "                         ('cat_nulls', HandleCategoricalNulls()), \\\n",
    "                         ('FunderInstTransformer', \\\n",
    "                          FunderInstTransformer(initial_chars = 3, \\\n",
    "                                                groups = 15,apply=True)), \\\n",
    "                         ('CatMultiLabelTransformer',\\\n",
    "                          CatMultiLabelTransformer(apply=True)) \\\n",
    "                         ])\n",
    "\n",
    "cat_pipeline_high_level_freq_based = Pipeline([ \\\n",
    "                         ('selector',DataFrameSelector(high_levels_cat_columns)), \\\n",
    "                         ('cat_nulls', HandleCategoricalNulls()), \\\n",
    "                         ('FreqBasedCategoricalBinning', \\\n",
    "                          FreqBasedCategoricalBinning(buckets=20,apply=True)), \\\n",
    "                         ('CatMultiLabelTransformer',CatMultiLabelTransformer()) \\\n",
    "                         ])\n",
    "                         \n",
    "cat_pipeline_high_level_resp_based = Pipeline([ \\\n",
    "                         ('selector',DataFrameSelector(high_levels_cat_columns)), \\\n",
    "                         ('cat_nulls', HandleCategoricalNulls()), \\\n",
    "                         ('RespBasedCategoricalBinning', \\\n",
    "                          RespBasedCategoricalBinning(buckets=20,apply=True)), \\\n",
    "                         ('CatMultiLabelTransformer',CatMultiLabelTransformer()) \\\n",
    "                         ])\n",
    "\n",
    "choose_high_level_cat_pipeline = Pipeline([ \\\n",
    "                                         ('ChooseCatPipelineType', ChooseCatPipelineType( \\\n",
    "                                         freq_pipeline = cat_pipeline_high_level_freq_based, \\\n",
    "                                         resp_pipeline = cat_pipeline_high_level_resp_based, \\\n",
    "                                         method = 'freq')) \\\n",
    "                                         ])\n",
    "                                         \n",
    "cat_pipeline_low_level = Pipeline([ \\\n",
    "                         ('selector',DataFrameSelector(low_levels_cat_columns)), \\\n",
    "                         ('cat_nulls', HandleCategoricalNulls()), \\\n",
    "                         ('CatMultiLabelTransformer',CatMultiLabelTransformer(apply=True)) \\\n",
    "                        ])\n",
    "\n",
    "#Combine all categorical pipelines first:\n",
    "full_categorical_pipeline = FeatureUnion(transformer_list = [ \\\n",
    "                                         (\"fuzz_pipeline\",fuzz_pipeline), \\\n",
    "                                         (\"choose_high_level_cat_pipeline\",\\\n",
    "                                          choose_high_level_cat_pipeline), \\\n",
    "                                         (\"cat_pipeline_low_level\", cat_pipeline_low_level) \\\n",
    "                                         ])       \n",
    "\n",
    "#amount_tsh pipelines\n",
    "amount_tsh_prep_pipeline = Pipeline([ \\\n",
    "                         ('selector',\\\n",
    "                          DataFrameSelector(['source_class', 'basin', \\\n",
    "                                             'waterpoint_type_group'])), \\\n",
    "                         ('cat_nulls', HandleCategoricalNulls()) \\\n",
    "                         ])\n",
    "\n",
    "amount_tsh_selector = Pipeline([('selector',DataFrameSelector(['amount_tsh']))])\n",
    "\n",
    "amount_tsh_transformer = Pipeline([ ('amount_tsh_prep',FeatureUnion(transformer_list = [ \\\n",
    "                                    (\"amount_tsh_prep_pipeline\",amount_tsh_prep_pipeline), \\\n",
    "                                    (\"amount_tsh_selector\",amount_tsh_selector)])) \\\n",
    "                                    ,(\"Numpy2DFTransformer\", \\\n",
    "                                      Numpy2DFTransformer(['source_class', \\\n",
    "                                                           'basin', \\\n",
    "                                                           'waterpoint_type_group',\\\n",
    "                                                           'amount_tsh'])) \\\n",
    "                                    ,('AmountTSHTransformer', \\\n",
    "                                      AmountTSHTransformer()) \\\n",
    "                                ])\n",
    "\n",
    "#age pipeline\n",
    "age_pipeline = Pipeline([('selector',DataFrameSelector(['date_recorded',\\\n",
    "                                                        'construction_year'])), \\\n",
    "                         ('YearTransformer',YearTransformer())])\n",
    "\n",
    "#Lat Long pipelines\n",
    "lat_long_prep_pipeline = Pipeline([('selector',DataFrameSelector(['lga',\\\n",
    "                                                        'region',\\\n",
    "                                                        'ward'])), \\\n",
    "                         ('cat_nulls', HandleCategoricalNulls())])\n",
    "\n",
    "lat_long_selector = Pipeline([('selector',DataFrameSelector(['longitude','latitude']))])\n",
    "\n",
    "lat_long_transformer = Pipeline([ ('lat_long_prep',\\\n",
    "                                   FeatureUnion(transformer_list = [ \\\n",
    "                                    (\"lat_long_prep_pipeline\",\\\n",
    "                                     lat_long_prep_pipeline), \\\n",
    "                                    (\"lat_long_selector\",\\\n",
    "                                     lat_long_selector)])) \\\n",
    "                                    ,(\"Numpy2DFTransformer\", \\\n",
    "                                      Numpy2DFTransformer(['lga','region',\\\n",
    "                                                           'ward','longitude',\\\n",
    "                                                           'latitude'])) \\\n",
    "                                    ,('LatitudeLongitudeProcess', \\\n",
    "                                      LatitudeLongitudeProcess(strategy=\"custom\")) \\\n",
    "                                ])\n",
    "#lat_long_transformer always return the \n",
    "#data in latitude, longitude order\n",
    "\n",
    "#gps_height pipelines.\n",
    "#This is dependent on the lat_long pipeline\n",
    "gps_height_transformer = Pipeline([('gps_height_prep',\\\n",
    "                                    FeatureUnion(transformer_list=[('lat_long_transformer', \\\n",
    "                                                                   lat_long_transformer), \\\n",
    "                                   ('gps_selector',DataFrameSelector(['gps_height']))]))\n",
    "                                   ,(\"Numpy2DFTransformer\", \\\n",
    "                                      Numpy2DFTransformer(['latitude','longitude','gps_height']))\n",
    "                                   ,('GPSHeightTransformer',GPSHeightTransformer(method='median'))\n",
    "#                                    ,('GPSHeightTransformer',GPSHeightTransformer(method='custom'))\n",
    "                                  ])\n",
    "#population pipelines.\n",
    "#This is dependent on the lat_long pipeline\n",
    "population_transformer = Pipeline([('population_prep',\\\n",
    "                                    FeatureUnion(transformer_list=[('lat_long_transformer', \\\n",
    "                                                                   lat_long_transformer), \\\n",
    "                                   ('population_selector',DataFrameSelector(['population']))]))\n",
    "                                   ,(\"Numpy2DFTransformer\", \\\n",
    "                                      Numpy2DFTransformer(['latitude','longitude','population']))\n",
    "                                   ,('PopulationTransformer',PopulationTransformer(method = 'ignore'))\n",
    "                                   #,('PopulationTransformer',PopulationTransformer(method = 'custom')) #NOT worth\n",
    "                                   # ,('PopulationTransformer',PopulationTransformer(method = 'median')) #NOT Worth\n",
    "                                  ])\n",
    "\n",
    "full_numeric_transformations = Pipeline([('all_numeric_transformations', \\\n",
    "                                        FeatureUnion(transformer_list = \\\n",
    "                                                     [('amount_tsh_transformer',amount_tsh_transformer), \\\n",
    "                                        ('lat_long_transformer',lat_long_transformer), \\\n",
    "                                        ('gps_height_transformer',gps_height_transformer), \\\n",
    "                                        ('population_transformer',population_transformer), \\\n",
    "                                        ('age_pipeline',age_pipeline) \\\n",
    "                                       ])) \\\n",
    "                                       ,('scaler',ScaleData())\n",
    "                                        ])\n",
    "\n",
    "all_transformations = Pipeline([ \\\n",
    "                                ('all_transformations', \\\n",
    "                                    FeatureUnion(transformer_list = \\\n",
    "                                                 [('full_categorical_pipeline',\\\n",
    "                                                   full_categorical_pipeline), \\\n",
    "                                                  ('full_numeric_transformations',\\\n",
    "                                                   full_numeric_transformations)])) \\\n",
    "                                 ])\n",
    "\n",
    "predict_pipeline = Pipeline([('all_transformations',all_transformations), \\\n",
    "                             ('rf',RandomForestClassifier(n_estimators=5000,n_jobs=-1))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = afr.drop(['defective',\"new_ids\"],axis=1)\n",
    "y = afr[\"defective\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "predict_pipeline = Pipeline([('all_transformations',all_transformations), \\\n",
    "                             ('rf',RandomForestClassifier())])\n",
    "params = {\"rf__n_estimators\": [500,1000,1500], \"rf__max_depth\": [10,13,15], \"rf__n_jobs\":[-1], \"rf__class_weight\": [\"balanced\",{\"no\":1,\"yes\":9}]}\n",
    "score = make_scorer(error)\n",
    "\n",
    "search = GridSearchCV(predict_pipeline, params, iid=False, cv=5, scoring=score, return_train_score=False)\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error =  0.06336194325180117\n",
      "test error =  0.08089358944908057\n",
      "train acc =  0.6425\n",
      "test acc =  0.6189\n"
     ]
    }
   ],
   "source": [
    "predict_pipeline = Pipeline([('all_transformations',all_transformations), \\\n",
    "                             ('rf',RandomForestClassifier(max_depth = 15, n_estimators = 1500,\n",
    "                                                          n_jobs = -1, class_weight = {'no': 1, 'yes': 9}))])\n",
    "predict_pipeline.fit(X_train,y_train)\n",
    "y_train_predict = predict_pipeline.predict(X_train)\n",
    "y_test_predict = predict_pipeline.predict(X_test)\n",
    "# y_train_predict = [1 if i > 0.5 else 0 for i in y_train_predict]\n",
    "# y_test_predict = [1 if i > 0.5 else 0 for i in y_test_predict]\n",
    "print(\"train error = \", error_met(y_train, y_train_predict))\n",
    "print(\"test error = \", error_met(y_test, y_test_predict))\n",
    "print(\"train acc = \", accuracy_score(y_train, y_train_predict))\n",
    "print(\"test acc = \", accuracy_score(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error =  0.06336194325180117\n",
      "test error =  0.08089358944908057\n",
      "train acc =  0.6425\n",
      "test acc =  0.6189\n"
     ]
    }
   ],
   "source": [
    "y_train_predict = predict_pipeline.predict_proba(X_train)[:,1]\n",
    "y_test_predict = predict_pipeline.predict_proba(X_test)[:,1]\n",
    "y_train_predict = [\"yes\" if i > 0.5 else \"no\" for i in y_train_predict]\n",
    "y_test_predict = [\"yes\" if i > 0.5 else \"no\" for i in y_test_predict]\n",
    "print(\"train error = \", error_met(y_train, y_train_predict))\n",
    "print(\"test error = \", error_met(y_test, y_test_predict))\n",
    "print(\"train acc = \", accuracy_score(y_train, y_train_predict))\n",
    "print(\"test acc = \", accuracy_score(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tr = y_train_predict\n",
    "rf_val = y_test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##final rf_model\n",
    "predict_pipeline.fit(X,y)\n",
    "test = afr_test.drop(\"new_ids\",axis=1)\n",
    "test = predict_pipeline.predict_proba(test)[:,1]\n",
    "test = [\"yes\" if i > 0.50 else \"no\" for i in test]\n",
    "rf_ts = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['subvillage','lga','ward','basin','region','district_code',\n",
    "                          'public_meeting','scheme_management',\n",
    "                          'permit','extraction_type_group',\n",
    "                          'management','payment_type',\n",
    "                          'water_quality','quantity_group',\n",
    "                          'source','waterpoint_type','funder','installer']\n",
    "\n",
    "all_transformations = Pipeline([ \\\n",
    "                                ('all_transformations', \\\n",
    "                                    FeatureUnion(transformer_list = \\\n",
    "                                                 [('full_numeric_transformations',\\\n",
    "                                                   full_numeric_transformations)])) \\\n",
    "                                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_transformed = all_transformations.fit_transform(X_train,y_train)\n",
    "all_columns = ['amount_tsh',\"latitude\",'longitude',\n",
    "               'gps_height','population','age']\n",
    "training_transformed = pd.DataFrame(training_transformed,columns = all_columns)\n",
    "\n",
    "training_transformed[cat_columns] = X_train.reset_index()[cat_columns]\n",
    "training_labels = y_train.map(lambda x: 0 if x == \"no\" else 1)\n",
    "\n",
    "training_transformed.fillna(\"NaN\",inplace=True)\n",
    "\n",
    "test_transformed = all_transformations.transform(X_test)\n",
    "test_transformed = pd.DataFrame(test_transformed,columns = all_columns)\n",
    "\n",
    "test_transformed[cat_columns] = X_test.reset_index()[cat_columns]\n",
    "test_labels = y_test.map(lambda x: 0 if x == \"no\" else 1)\n",
    "test_transformed.fillna(\"NaN\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.conda/envs/myenv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-30-7e525438f186>\", line 18, in <module>\n",
      "    scores = ct.cv(pool, params)\n",
      "  File \"/home/ubuntu/.conda/envs/myenv/lib/python3.7/site-packages/catboost/core.py\", line 2951, in cv\n",
      "    as_pandas, max_time_spent_on_fixed_cost_ratio, dev_max_iterations_batch_size)\n",
      "  File \"_catboost.pyx\", line 2665, in _catboost._cv\n",
      "  File \"_catboost.pyx\", line 2683, in _catboost._cv\n",
      "_catboost.CatboostError: catboost/cuda/cuda_lib/cuda_base.h:268: CUDA error 35: CUDA driver version is insufficient for CUDA runtime version\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.conda/envs/myenv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2018, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'CatboostError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.conda/envs/myenv/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ubuntu/.conda/envs/myenv/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.conda/envs/myenv/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/ubuntu/.conda/envs/myenv/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/ubuntu/.conda/envs/myenv/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/ubuntu/.conda/envs/myenv/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/ubuntu/.conda/envs/myenv/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/ubuntu/.conda/envs/myenv/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/ubuntu/.conda/envs/myenv/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/ubuntu/.conda/envs/myenv/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "CatboostError",
     "evalue": "catboost/cuda/cuda_lib/cuda_base.h:268: CUDA error 35: CUDA driver version is insufficient for CUDA runtime version",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "## for parameter selection\n",
    "cat_features = list(range(6,24))\n",
    "pool = ct.Pool(training_transformed, training_labels, cat_features)\n",
    "s = []\n",
    "\n",
    "for d in [9]:\n",
    "    for trees in [250]:\n",
    "        for lr in [0.1]:\n",
    "            params = {\"iterations\": trees,\n",
    "                    'depth': d,\n",
    "                    'verbose': False,\n",
    "                    \"loss_function\": \"Logloss\",\n",
    "                    \"task_type\": 'GPU',\n",
    "                     \"learning_rate\": lr,\n",
    "                     \"class_weights\": [1,8],\n",
    "                     \"stratified\":True,\n",
    "                     \"fold_count\":5}\n",
    "            scores = ct.cv(pool, params)\n",
    "            s = [trees, d, lr, scores.iloc[-1,:].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error =  0.05155352287606155\n",
      "test error =  0.08177263357846798\n",
      "train acc =  0.756725\n",
      "test acc =  0.7016\n"
     ]
    }
   ],
   "source": [
    "cat_features = list(range(6,24))\n",
    "model = ct.CatBoostClassifier(iterations = 250, depth = 9, verbose = False, loss_function = \"Logloss\",\n",
    "                              use_best_model = False, learning_rate = 0.1, class_weights = [1,8])\n",
    "# Fit model\n",
    "model.fit(training_transformed, training_labels, cat_features)\n",
    "# Get predicted classes\n",
    "y_train_predict = model.predict(training_transformed)\n",
    "y_test_predict = model.predict(test_transformed)\n",
    "print(\"train error = \", error_met(training_labels, y_train_predict))\n",
    "print(\"test error = \", error_met(test_labels, y_test_predict))\n",
    "print(\"train acc = \", accuracy_score(training_labels, y_train_predict))\n",
    "print(\"test acc = \", accuracy_score(test_labels, y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error =  0.055596726566498514\n",
      "test error =  0.07679182989804885\n",
      "train acc =  0.699675\n",
      "test acc =  0.6558\n"
     ]
    }
   ],
   "source": [
    "y_train_predict = [1 if i > 0.4 else 0 for i in model.predict_proba(training_transformed)[:,1]]\n",
    "y_test_predict = [1 if i > 0.4 else 0 for i in model.predict_proba(test_transformed)[:,1]]\n",
    "print(\"train error = \", error_met(training_labels, y_train_predict))\n",
    "print(\"test error = \", error_met(test_labels, y_test_predict))\n",
    "print(\"train acc = \", accuracy_score(training_labels, y_train_predict))\n",
    "print(\"test acc = \", accuracy_score(test_labels, y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_tr = y_train_predict\n",
    "ct_val = y_test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = afr.drop(\"new_ids\",axis=1)\n",
    "train = all_transformations.transform(train)\n",
    "train = pd.DataFrame(train,columns=all_columns)\n",
    "train[cat_columns] = afr.reset_index()[cat_columns]\n",
    "train.fillna(\"NaN\",inplace=True)\n",
    "\n",
    "model.fit(train, afr.defective.map(lambda x: 1 if x == \"yes\" else 0), cat_features)\n",
    "\n",
    "test = afr_test.drop(\"new_ids\",axis=1)\n",
    "test = all_transformations.transform(test)\n",
    "test = pd.DataFrame(test,columns=all_columns)\n",
    "test[cat_columns] = afr_test.reset_index()[cat_columns]\n",
    "test.fillna(\"NaN\",inplace=True)\n",
    "ct_ts = [1 if i > 0.4 else 0 for i in model.predict_proba(test)[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = 0\n",
    "# t = [t+1 if ((x == \"yes\") and (y == 1)) or ((x == \"no\") and (y == 0)) else t for x,y in zip(rf_ts,ct_ts)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = [1 if x == \"yes\" else 0 for x in rf_ts]\n",
    "# # (np.array(t)*np.array(ct_ts)).sum()\n",
    "# c = 0\n",
    "# for i in range(len(ct_ts)):\n",
    "#     if ct_ts[i] == t[i]:\n",
    "#         c+=1\n",
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = predict_pipeline.predict_proba(afr_test.drop(\"new_ids\",axis=1))[:,1]\n",
    "p2 = model.predict_proba(test)[:,1]\n",
    "# l = [1 if p > 0.5 else 0 for p in (p1+p2)/2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: CatBoost with different imputations and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = afr\n",
    "test = afr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns_missing = ['funder', 'installer', 'subvillage', 'scheme_management', 'scheme_name']\n",
    "bool_columns_missing = ['public_meeting', 'permit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replacing missing in categorical by field \"Missing\"\n",
    "for col in cat_columns_missing:\n",
    "    raw_df[col] = raw_df[col].fillna(\"Missing\")\n",
    "    test[col] = test[col].fillna(\"Missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replacing missing in boolean columns by 99\n",
    "raw_df[bool_columns_missing] *= 1\n",
    "test[bool_columns_missing] *= 1\n",
    "\n",
    "for col in bool_columns_missing:\n",
    "    raw_df[col] = raw_df[col].fillna(99)\n",
    "    test[col] = test[col].fillna(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting year, month, day out of date recorded\n",
    "date_col = ['date_recorded']\n",
    "\n",
    "for col in date_col:\n",
    "    raw_df[col] = pd.to_datetime(raw_df[col])\n",
    "    test[col] = pd.to_datetime(test[col])\n",
    "    \n",
    "    raw_df[col+'year'] = raw_df[col].dt.year\n",
    "    raw_df[col+'month'] = raw_df[col].dt.month\n",
    "    raw_df[col+'day'] = raw_df[col].dt.day\n",
    "    \n",
    "    test[col+'year'] = test[col].dt.year\n",
    "    test[col+'month'] = test[col].dt.month\n",
    "    test[col+'day'] = test[col].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Coverting target to 1,0\n",
    "raw_df[\"defective\"] = raw_df[\"defective\"].map(lambda x: 1 if x==\"yes\" else 0)\n",
    "test[\"defective\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting num_private to boolean as 99% are 0's\n",
    "raw_df[\"num_private\"] = raw_df[\"num_private\"].map(lambda x: 0 if x==0 else 1)\n",
    "test[\"num_private\"] = test[\"num_private\"].map(lambda x: 0 if x==0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Derived column = construction year - recorded year\n",
    "raw_df[\"construction_minus_recorded\"] = raw_df[\"date_recordedyear\"] - raw_df[\"construction_year\"]\n",
    "raw_df.loc[raw_df[\"construction_year\"]==0,'construction_minus_recorded'] = 99999\n",
    "test[\"construction_minus_recorded\"] = test[\"date_recordedyear\"] - test[\"construction_year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_95 = pd.DataFrame(raw_df.amount_tsh.describe(percentiles=[0.95])).iloc[5][0]\n",
    "raw_df.loc[raw_df[\"amount_tsh\"]>perc_95,'amount_tsh'] = perc_95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['new_ids', 'region_code', 'district_code', 'recorded_by', 'construction_year', \n",
    "                'date_recordedyear', 'date_recordedmonth', 'date_recordedday', 'wpt_name'] + date_col\n",
    "raw_df = raw_df.drop(cols_to_drop,axis=1)\n",
    "test = test.drop(cols_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['amount_tsh', 'funder', 'gps_height', 'installer', 'longitude',\n",
       "       'latitude', 'num_private', 'basin', 'subvillage', 'region', 'lga',\n",
       "       'ward', 'population', 'public_meeting', 'scheme_management',\n",
       "       'scheme_name', 'permit', 'extraction_type', 'extraction_type_group',\n",
       "       'extraction_type_class', 'management', 'management_group', 'payment',\n",
       "       'payment_type', 'water_quality', 'quality_group', 'quantity',\n",
       "       'quantity_group', 'source', 'source_type', 'source_class',\n",
       "       'waterpoint_type', 'waterpoint_type_group', 'defective',\n",
       "       'construction_minus_recorded'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = raw_df.drop(\"defective\", axis=1)\n",
    "y = raw_df[\"defective\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(X_train.dtypes).reset_index()\n",
    "cat_features = list(temp.index[temp[0]==\"object\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5895366\ttotal: 203ms\tremaining: 50.7s\n",
      "1:\tlearn: 0.5259532\ttotal: 374ms\tremaining: 46.4s\n",
      "2:\tlearn: 0.4663044\ttotal: 606ms\tremaining: 49.9s\n",
      "3:\tlearn: 0.4320168\ttotal: 845ms\tremaining: 52s\n",
      "4:\tlearn: 0.4037437\ttotal: 940ms\tremaining: 46.1s\n",
      "5:\tlearn: 0.3771304\ttotal: 1.19s\tremaining: 48.5s\n",
      "6:\tlearn: 0.3598724\ttotal: 1.38s\tremaining: 48s\n",
      "7:\tlearn: 0.3445778\ttotal: 1.63s\tremaining: 49.2s\n",
      "8:\tlearn: 0.3342532\ttotal: 1.86s\tremaining: 49.9s\n",
      "9:\tlearn: 0.3258657\ttotal: 2.11s\tremaining: 50.7s\n",
      "10:\tlearn: 0.3196463\ttotal: 2.22s\tremaining: 48.2s\n",
      "11:\tlearn: 0.3113043\ttotal: 2.47s\tremaining: 49s\n",
      "12:\tlearn: 0.3055328\ttotal: 2.71s\tremaining: 49.5s\n",
      "13:\tlearn: 0.3001747\ttotal: 2.95s\tremaining: 49.8s\n",
      "14:\tlearn: 0.2951696\ttotal: 3.05s\tremaining: 47.9s\n",
      "15:\tlearn: 0.2908993\ttotal: 3.29s\tremaining: 48s\n",
      "16:\tlearn: 0.2871501\ttotal: 3.53s\tremaining: 48.3s\n",
      "17:\tlearn: 0.2844886\ttotal: 3.8s\tremaining: 49s\n",
      "18:\tlearn: 0.2822904\ttotal: 3.97s\tremaining: 48.3s\n",
      "19:\tlearn: 0.2806250\ttotal: 4.12s\tremaining: 47.3s\n",
      "20:\tlearn: 0.2793495\ttotal: 4.22s\tremaining: 46s\n",
      "21:\tlearn: 0.2780549\ttotal: 4.33s\tremaining: 44.8s\n",
      "22:\tlearn: 0.2768593\ttotal: 4.59s\tremaining: 45.3s\n",
      "23:\tlearn: 0.2753399\ttotal: 4.71s\tremaining: 44.3s\n",
      "24:\tlearn: 0.2740664\ttotal: 4.87s\tremaining: 43.8s\n",
      "25:\tlearn: 0.2729804\ttotal: 5.1s\tremaining: 43.9s\n",
      "26:\tlearn: 0.2729616\ttotal: 5.12s\tremaining: 42.3s\n",
      "27:\tlearn: 0.2722274\ttotal: 5.19s\tremaining: 41.2s\n",
      "28:\tlearn: 0.2710224\ttotal: 5.28s\tremaining: 40.2s\n",
      "29:\tlearn: 0.2703086\ttotal: 5.38s\tremaining: 39.4s\n",
      "30:\tlearn: 0.2698467\ttotal: 5.46s\tremaining: 38.6s\n",
      "31:\tlearn: 0.2690183\ttotal: 5.68s\tremaining: 38.7s\n",
      "32:\tlearn: 0.2685215\ttotal: 5.73s\tremaining: 37.7s\n",
      "33:\tlearn: 0.2676845\ttotal: 5.99s\tremaining: 38s\n",
      "34:\tlearn: 0.2669929\ttotal: 6.19s\tremaining: 38s\n",
      "35:\tlearn: 0.2660441\ttotal: 6.44s\tremaining: 38.3s\n",
      "36:\tlearn: 0.2651400\ttotal: 6.66s\tremaining: 38.3s\n",
      "37:\tlearn: 0.2643162\ttotal: 6.8s\tremaining: 37.9s\n",
      "38:\tlearn: 0.2632663\ttotal: 6.93s\tremaining: 37.5s\n",
      "39:\tlearn: 0.2632632\ttotal: 6.96s\tremaining: 36.6s\n",
      "40:\tlearn: 0.2628402\ttotal: 7.07s\tremaining: 36.1s\n",
      "41:\tlearn: 0.2628385\ttotal: 7.09s\tremaining: 35.1s\n",
      "42:\tlearn: 0.2628283\ttotal: 7.12s\tremaining: 34.3s\n",
      "43:\tlearn: 0.2626221\ttotal: 7.35s\tremaining: 34.4s\n",
      "44:\tlearn: 0.2626206\ttotal: 7.38s\tremaining: 33.6s\n",
      "45:\tlearn: 0.2619422\ttotal: 7.61s\tremaining: 33.7s\n",
      "46:\tlearn: 0.2617303\ttotal: 7.66s\tremaining: 33.1s\n",
      "47:\tlearn: 0.2614968\ttotal: 7.88s\tremaining: 33.2s\n",
      "48:\tlearn: 0.2613001\ttotal: 7.93s\tremaining: 32.6s\n",
      "49:\tlearn: 0.2609312\ttotal: 8.2s\tremaining: 32.8s\n",
      "50:\tlearn: 0.2602477\ttotal: 8.47s\tremaining: 33.1s\n",
      "51:\tlearn: 0.2595524\ttotal: 8.73s\tremaining: 33.3s\n",
      "52:\tlearn: 0.2590226\ttotal: 9s\tremaining: 33.4s\n",
      "53:\tlearn: 0.2585328\ttotal: 9.26s\tremaining: 33.6s\n",
      "54:\tlearn: 0.2581651\ttotal: 9.53s\tremaining: 33.8s\n",
      "55:\tlearn: 0.2577512\ttotal: 9.8s\tremaining: 34s\n",
      "56:\tlearn: 0.2575776\ttotal: 9.88s\tremaining: 33.5s\n",
      "57:\tlearn: 0.2571938\ttotal: 10.1s\tremaining: 33.5s\n",
      "58:\tlearn: 0.2569653\ttotal: 10.3s\tremaining: 33.2s\n",
      "59:\tlearn: 0.2564669\ttotal: 10.5s\tremaining: 33.3s\n",
      "60:\tlearn: 0.2559743\ttotal: 10.7s\tremaining: 33.2s\n",
      "61:\tlearn: 0.2551519\ttotal: 11s\tremaining: 33.3s\n",
      "62:\tlearn: 0.2547532\ttotal: 11.2s\tremaining: 33.2s\n",
      "63:\tlearn: 0.2546861\ttotal: 11.3s\tremaining: 32.7s\n",
      "64:\tlearn: 0.2543077\ttotal: 11.5s\tremaining: 32.7s\n",
      "65:\tlearn: 0.2539768\ttotal: 11.7s\tremaining: 32.7s\n",
      "66:\tlearn: 0.2534021\ttotal: 12s\tremaining: 32.8s\n",
      "67:\tlearn: 0.2533175\ttotal: 12.1s\tremaining: 32.3s\n",
      "68:\tlearn: 0.2531496\ttotal: 12.1s\tremaining: 31.8s\n",
      "69:\tlearn: 0.2531052\ttotal: 12.2s\tremaining: 31.4s\n",
      "70:\tlearn: 0.2530287\ttotal: 12.3s\tremaining: 31.1s\n",
      "71:\tlearn: 0.2530281\ttotal: 12.4s\tremaining: 30.5s\n",
      "72:\tlearn: 0.2527381\ttotal: 12.5s\tremaining: 30.4s\n",
      "73:\tlearn: 0.2527121\ttotal: 12.6s\tremaining: 29.9s\n",
      "74:\tlearn: 0.2525914\ttotal: 12.7s\tremaining: 29.7s\n",
      "75:\tlearn: 0.2525040\ttotal: 12.8s\tremaining: 29.3s\n",
      "76:\tlearn: 0.2524700\ttotal: 12.8s\tremaining: 28.9s\n",
      "77:\tlearn: 0.2523171\ttotal: 13.1s\tremaining: 28.8s\n",
      "78:\tlearn: 0.2522726\ttotal: 13.2s\tremaining: 28.5s\n",
      "79:\tlearn: 0.2522631\ttotal: 13.2s\tremaining: 28.1s\n",
      "80:\tlearn: 0.2522569\ttotal: 13.3s\tremaining: 27.6s\n",
      "81:\tlearn: 0.2520341\ttotal: 13.3s\tremaining: 27.3s\n",
      "82:\tlearn: 0.2518552\ttotal: 13.4s\tremaining: 27s\n",
      "83:\tlearn: 0.2518473\ttotal: 13.4s\tremaining: 26.5s\n",
      "84:\tlearn: 0.2514564\ttotal: 13.7s\tremaining: 26.5s\n",
      "85:\tlearn: 0.2513335\ttotal: 13.9s\tremaining: 26.5s\n",
      "86:\tlearn: 0.2511725\ttotal: 14.1s\tremaining: 26.5s\n",
      "87:\tlearn: 0.2511576\ttotal: 14.2s\tremaining: 26.1s\n",
      "88:\tlearn: 0.2509196\ttotal: 14.5s\tremaining: 26.1s\n",
      "89:\tlearn: 0.2507391\ttotal: 14.6s\tremaining: 26s\n",
      "90:\tlearn: 0.2506682\ttotal: 14.7s\tremaining: 25.7s\n",
      "91:\tlearn: 0.2506223\ttotal: 14.7s\tremaining: 25.3s\n",
      "92:\tlearn: 0.2506222\ttotal: 14.8s\tremaining: 24.9s\n",
      "93:\tlearn: 0.2506040\ttotal: 14.8s\tremaining: 24.6s\n",
      "94:\tlearn: 0.2506039\ttotal: 14.8s\tremaining: 24.2s\n",
      "95:\tlearn: 0.2503106\ttotal: 15s\tremaining: 24s\n",
      "96:\tlearn: 0.2500449\ttotal: 15.2s\tremaining: 24s\n",
      "97:\tlearn: 0.2498743\ttotal: 15.4s\tremaining: 23.8s\n",
      "98:\tlearn: 0.2498736\ttotal: 15.4s\tremaining: 23.5s\n",
      "99:\tlearn: 0.2496731\ttotal: 15.6s\tremaining: 23.4s\n",
      "100:\tlearn: 0.2496729\ttotal: 15.7s\tremaining: 23.1s\n",
      "101:\tlearn: 0.2496724\ttotal: 15.7s\tremaining: 22.8s\n",
      "102:\tlearn: 0.2493782\ttotal: 15.9s\tremaining: 22.7s\n",
      "103:\tlearn: 0.2491257\ttotal: 16s\tremaining: 22.5s\n",
      "104:\tlearn: 0.2491233\ttotal: 16.1s\tremaining: 22.2s\n",
      "105:\tlearn: 0.2491214\ttotal: 16.1s\tremaining: 21.9s\n",
      "106:\tlearn: 0.2487374\ttotal: 16.4s\tremaining: 21.9s\n",
      "107:\tlearn: 0.2484628\ttotal: 16.6s\tremaining: 21.9s\n",
      "108:\tlearn: 0.2484266\ttotal: 16.7s\tremaining: 21.6s\n",
      "109:\tlearn: 0.2481849\ttotal: 16.9s\tremaining: 21.6s\n",
      "110:\tlearn: 0.2481833\ttotal: 17s\tremaining: 21.3s\n",
      "111:\tlearn: 0.2478301\ttotal: 17.1s\tremaining: 21.1s\n",
      "112:\tlearn: 0.2475187\ttotal: 17.3s\tremaining: 21s\n",
      "113:\tlearn: 0.2475168\ttotal: 17.4s\tremaining: 20.7s\n",
      "114:\tlearn: 0.2471910\ttotal: 17.6s\tremaining: 20.7s\n",
      "115:\tlearn: 0.2469421\ttotal: 17.9s\tremaining: 20.7s\n",
      "116:\tlearn: 0.2466629\ttotal: 18s\tremaining: 20.4s\n",
      "117:\tlearn: 0.2465532\ttotal: 18.2s\tremaining: 20.4s\n",
      "118:\tlearn: 0.2465343\ttotal: 18.4s\tremaining: 20.3s\n",
      "119:\tlearn: 0.2463119\ttotal: 18.7s\tremaining: 20.2s\n",
      "120:\tlearn: 0.2461755\ttotal: 18.9s\tremaining: 20.2s\n",
      "121:\tlearn: 0.2453060\ttotal: 19.2s\tremaining: 20.1s\n",
      "122:\tlearn: 0.2449583\ttotal: 19.4s\tremaining: 20.1s\n",
      "123:\tlearn: 0.2447909\ttotal: 19.7s\tremaining: 20s\n",
      "124:\tlearn: 0.2443545\ttotal: 19.9s\tremaining: 19.9s\n",
      "125:\tlearn: 0.2442015\ttotal: 20.1s\tremaining: 19.8s\n",
      "126:\tlearn: 0.2436503\ttotal: 20.4s\tremaining: 19.7s\n",
      "127:\tlearn: 0.2433083\ttotal: 20.6s\tremaining: 19.7s\n",
      "128:\tlearn: 0.2430033\ttotal: 20.9s\tremaining: 19.6s\n",
      "129:\tlearn: 0.2427908\ttotal: 21.2s\tremaining: 19.5s\n",
      "130:\tlearn: 0.2426054\ttotal: 21.4s\tremaining: 19.4s\n",
      "131:\tlearn: 0.2423144\ttotal: 21.7s\tremaining: 19.4s\n",
      "132:\tlearn: 0.2422460\ttotal: 21.9s\tremaining: 19.3s\n",
      "133:\tlearn: 0.2419272\ttotal: 22.2s\tremaining: 19.2s\n",
      "134:\tlearn: 0.2411382\ttotal: 22.4s\tremaining: 19.1s\n",
      "135:\tlearn: 0.2407820\ttotal: 22.7s\tremaining: 19s\n",
      "136:\tlearn: 0.2406123\ttotal: 22.9s\tremaining: 18.9s\n",
      "137:\tlearn: 0.2404247\ttotal: 23.2s\tremaining: 18.8s\n",
      "138:\tlearn: 0.2402484\ttotal: 23.4s\tremaining: 18.7s\n",
      "139:\tlearn: 0.2400195\ttotal: 23.7s\tremaining: 18.6s\n",
      "140:\tlearn: 0.2394723\ttotal: 23.9s\tremaining: 18.5s\n",
      "141:\tlearn: 0.2391927\ttotal: 24.1s\tremaining: 18.4s\n",
      "142:\tlearn: 0.2391493\ttotal: 24.4s\tremaining: 18.2s\n",
      "143:\tlearn: 0.2390515\ttotal: 24.6s\tremaining: 18.1s\n",
      "144:\tlearn: 0.2387597\ttotal: 24.9s\tremaining: 18s\n",
      "145:\tlearn: 0.2385378\ttotal: 25.1s\tremaining: 17.9s\n",
      "146:\tlearn: 0.2383542\ttotal: 25.4s\tremaining: 17.8s\n",
      "147:\tlearn: 0.2381806\ttotal: 25.6s\tremaining: 17.6s\n",
      "148:\tlearn: 0.2381658\ttotal: 25.9s\tremaining: 17.5s\n",
      "149:\tlearn: 0.2378285\ttotal: 26.1s\tremaining: 17.4s\n",
      "150:\tlearn: 0.2375483\ttotal: 26.4s\tremaining: 17.3s\n",
      "151:\tlearn: 0.2375320\ttotal: 26.6s\tremaining: 17.2s\n",
      "152:\tlearn: 0.2372416\ttotal: 26.8s\tremaining: 17s\n",
      "153:\tlearn: 0.2371083\ttotal: 27.1s\tremaining: 16.9s\n",
      "154:\tlearn: 0.2368866\ttotal: 27.3s\tremaining: 16.7s\n",
      "155:\tlearn: 0.2368367\ttotal: 27.5s\tremaining: 16.6s\n",
      "156:\tlearn: 0.2367275\ttotal: 27.8s\tremaining: 16.5s\n",
      "157:\tlearn: 0.2365335\ttotal: 28s\tremaining: 16.3s\n",
      "158:\tlearn: 0.2362685\ttotal: 28.3s\tremaining: 16.2s\n",
      "159:\tlearn: 0.2360693\ttotal: 28.5s\tremaining: 16s\n",
      "160:\tlearn: 0.2357981\ttotal: 28.7s\tremaining: 15.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161:\tlearn: 0.2352546\ttotal: 29s\tremaining: 15.7s\n",
      "162:\tlearn: 0.2349918\ttotal: 29.2s\tremaining: 15.6s\n",
      "163:\tlearn: 0.2345400\ttotal: 29.5s\tremaining: 15.5s\n",
      "164:\tlearn: 0.2344454\ttotal: 29.7s\tremaining: 15.3s\n",
      "165:\tlearn: 0.2342619\ttotal: 30s\tremaining: 15.2s\n",
      "166:\tlearn: 0.2340629\ttotal: 30.3s\tremaining: 15s\n",
      "167:\tlearn: 0.2338317\ttotal: 30.5s\tremaining: 14.9s\n",
      "168:\tlearn: 0.2334834\ttotal: 30.8s\tremaining: 14.7s\n",
      "169:\tlearn: 0.2334626\ttotal: 31s\tremaining: 14.6s\n",
      "170:\tlearn: 0.2332320\ttotal: 31.3s\tremaining: 14.4s\n",
      "171:\tlearn: 0.2331167\ttotal: 31.5s\tremaining: 14.3s\n",
      "172:\tlearn: 0.2330491\ttotal: 31.7s\tremaining: 14.1s\n",
      "173:\tlearn: 0.2329220\ttotal: 32s\tremaining: 14s\n",
      "174:\tlearn: 0.2326123\ttotal: 32.2s\tremaining: 13.8s\n",
      "175:\tlearn: 0.2321452\ttotal: 32.5s\tremaining: 13.7s\n",
      "176:\tlearn: 0.2318759\ttotal: 32.8s\tremaining: 13.5s\n",
      "177:\tlearn: 0.2317455\ttotal: 33s\tremaining: 13.3s\n",
      "178:\tlearn: 0.2316074\ttotal: 33.3s\tremaining: 13.2s\n",
      "179:\tlearn: 0.2315217\ttotal: 33.5s\tremaining: 13s\n",
      "180:\tlearn: 0.2314142\ttotal: 33.7s\tremaining: 12.9s\n",
      "181:\tlearn: 0.2313608\ttotal: 34s\tremaining: 12.7s\n",
      "182:\tlearn: 0.2312359\ttotal: 34.2s\tremaining: 12.5s\n",
      "183:\tlearn: 0.2310986\ttotal: 34.5s\tremaining: 12.4s\n",
      "184:\tlearn: 0.2308630\ttotal: 34.7s\tremaining: 12.2s\n",
      "185:\tlearn: 0.2308335\ttotal: 34.9s\tremaining: 12s\n",
      "186:\tlearn: 0.2308145\ttotal: 35.1s\tremaining: 11.8s\n",
      "187:\tlearn: 0.2305472\ttotal: 35.4s\tremaining: 11.7s\n",
      "188:\tlearn: 0.2304540\ttotal: 35.6s\tremaining: 11.5s\n",
      "189:\tlearn: 0.2303956\ttotal: 35.9s\tremaining: 11.3s\n",
      "190:\tlearn: 0.2300859\ttotal: 36.1s\tremaining: 11.2s\n",
      "191:\tlearn: 0.2300360\ttotal: 36.4s\tremaining: 11s\n",
      "192:\tlearn: 0.2300029\ttotal: 36.6s\tremaining: 10.8s\n",
      "193:\tlearn: 0.2299028\ttotal: 36.9s\tremaining: 10.6s\n",
      "194:\tlearn: 0.2297516\ttotal: 37.1s\tremaining: 10.5s\n",
      "195:\tlearn: 0.2296344\ttotal: 37.4s\tremaining: 10.3s\n",
      "196:\tlearn: 0.2295994\ttotal: 37.6s\tremaining: 10.1s\n",
      "197:\tlearn: 0.2295477\ttotal: 37.8s\tremaining: 9.94s\n",
      "198:\tlearn: 0.2294711\ttotal: 38.1s\tremaining: 9.76s\n",
      "199:\tlearn: 0.2294130\ttotal: 38.3s\tremaining: 9.59s\n",
      "200:\tlearn: 0.2290604\ttotal: 38.6s\tremaining: 9.41s\n",
      "201:\tlearn: 0.2287277\ttotal: 38.9s\tremaining: 9.24s\n",
      "202:\tlearn: 0.2286233\ttotal: 39.1s\tremaining: 9.06s\n",
      "203:\tlearn: 0.2285923\ttotal: 39.4s\tremaining: 8.88s\n",
      "204:\tlearn: 0.2284234\ttotal: 39.6s\tremaining: 8.69s\n",
      "205:\tlearn: 0.2283747\ttotal: 39.8s\tremaining: 8.51s\n",
      "206:\tlearn: 0.2282715\ttotal: 40.1s\tremaining: 8.33s\n",
      "207:\tlearn: 0.2282333\ttotal: 40.3s\tremaining: 8.14s\n",
      "208:\tlearn: 0.2281928\ttotal: 40.6s\tremaining: 7.96s\n",
      "209:\tlearn: 0.2281157\ttotal: 40.8s\tremaining: 7.77s\n",
      "210:\tlearn: 0.2280944\ttotal: 41s\tremaining: 7.58s\n",
      "211:\tlearn: 0.2279567\ttotal: 41.3s\tremaining: 7.39s\n",
      "212:\tlearn: 0.2279028\ttotal: 41.5s\tremaining: 7.21s\n",
      "213:\tlearn: 0.2276957\ttotal: 41.8s\tremaining: 7.03s\n",
      "214:\tlearn: 0.2276767\ttotal: 42s\tremaining: 6.84s\n",
      "215:\tlearn: 0.2276485\ttotal: 42.2s\tremaining: 6.65s\n",
      "216:\tlearn: 0.2275615\ttotal: 42.5s\tremaining: 6.46s\n",
      "217:\tlearn: 0.2274285\ttotal: 42.7s\tremaining: 6.27s\n",
      "218:\tlearn: 0.2272740\ttotal: 43s\tremaining: 6.08s\n",
      "219:\tlearn: 0.2272315\ttotal: 43.2s\tremaining: 5.88s\n",
      "220:\tlearn: 0.2270784\ttotal: 43.4s\tremaining: 5.7s\n",
      "221:\tlearn: 0.2269573\ttotal: 43.7s\tremaining: 5.51s\n",
      "222:\tlearn: 0.2267886\ttotal: 43.9s\tremaining: 5.32s\n",
      "223:\tlearn: 0.2267320\ttotal: 44.2s\tremaining: 5.13s\n",
      "224:\tlearn: 0.2267000\ttotal: 44.4s\tremaining: 4.93s\n",
      "225:\tlearn: 0.2265763\ttotal: 44.6s\tremaining: 4.74s\n",
      "226:\tlearn: 0.2263973\ttotal: 44.9s\tremaining: 4.55s\n",
      "227:\tlearn: 0.2263421\ttotal: 45.1s\tremaining: 4.35s\n",
      "228:\tlearn: 0.2263047\ttotal: 45.4s\tremaining: 4.16s\n",
      "229:\tlearn: 0.2262377\ttotal: 45.6s\tremaining: 3.97s\n",
      "230:\tlearn: 0.2261110\ttotal: 45.9s\tremaining: 3.77s\n",
      "231:\tlearn: 0.2260715\ttotal: 46.1s\tremaining: 3.58s\n",
      "232:\tlearn: 0.2259216\ttotal: 46.4s\tremaining: 3.38s\n",
      "233:\tlearn: 0.2257254\ttotal: 46.6s\tremaining: 3.19s\n",
      "234:\tlearn: 0.2256944\ttotal: 46.9s\tremaining: 2.99s\n",
      "235:\tlearn: 0.2255721\ttotal: 47.1s\tremaining: 2.79s\n",
      "236:\tlearn: 0.2255564\ttotal: 47.4s\tremaining: 2.6s\n",
      "237:\tlearn: 0.2255497\ttotal: 47.6s\tremaining: 2.4s\n",
      "238:\tlearn: 0.2255169\ttotal: 47.9s\tremaining: 2.2s\n",
      "239:\tlearn: 0.2254002\ttotal: 48.1s\tremaining: 2s\n",
      "240:\tlearn: 0.2252928\ttotal: 48.3s\tremaining: 1.8s\n",
      "241:\tlearn: 0.2251489\ttotal: 48.6s\tremaining: 1.61s\n",
      "242:\tlearn: 0.2248604\ttotal: 48.9s\tremaining: 1.41s\n",
      "243:\tlearn: 0.2248291\ttotal: 49.1s\tremaining: 1.21s\n",
      "244:\tlearn: 0.2246879\ttotal: 49.4s\tremaining: 1.01s\n",
      "245:\tlearn: 0.2246781\ttotal: 49.6s\tremaining: 807ms\n",
      "246:\tlearn: 0.2246254\ttotal: 49.9s\tremaining: 605ms\n",
      "247:\tlearn: 0.2245744\ttotal: 50.1s\tremaining: 404ms\n",
      "248:\tlearn: 0.2245102\ttotal: 50.3s\tremaining: 202ms\n",
      "249:\tlearn: 0.2244744\ttotal: 50.6s\tremaining: 0us\n",
      "0.08059177306046332\n",
      "0.050616803765179796\n"
     ]
    }
   ],
   "source": [
    "model = ct.CatBoostClassifier(iterations=250, \n",
    "                           depth=9, \n",
    "                           class_weights = [1,8],\n",
    "                           learning_rate=0.1,\n",
    "                           loss_function='Logloss')\n",
    "\n",
    "model.fit(X_train, y_train,cat_features=cat_features)\n",
    "\n",
    "preds_cat_val = model.predict(X_val)\n",
    "preds_cat_train = model.predict(X_train)\n",
    "\n",
    "#print(confusion_matrix(y_val, preds_cat))\n",
    "print(error_met(y_val, preds_cat_val))\n",
    "print(error_met(y_train, preds_cat_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error =  0.0548474580194481\n",
      "test error =  0.07710409873865429\n",
      "train acc =  0.7029333333333333\n",
      "test acc =  0.65256\n"
     ]
    }
   ],
   "source": [
    "y_train_predict = model.predict_proba(X_train)[:,1]\n",
    "y_test_predict = model.predict_proba(X_val)[:,1]\n",
    "y_train_predict = [1 if i > 0.4 else 0 for i in y_train_predict]\n",
    "y_test_predict = [1 if i > 0.4 else 0 for i in y_test_predict]\n",
    "print(\"train error = \", error_met(y_train, y_train_predict))\n",
    "print(\"test error = \", error_met(y_val, y_test_predict))\n",
    "print(\"train acc = \", accuracy_score(y_train, y_train_predict))\n",
    "print(\"test acc = \", accuracy_score(y_val, y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5898661\ttotal: 126ms\tremaining: 31.4s\n",
      "1:\tlearn: 0.5177157\ttotal: 241ms\tremaining: 29.8s\n",
      "2:\tlearn: 0.4732768\ttotal: 277ms\tremaining: 22.8s\n",
      "3:\tlearn: 0.4393138\ttotal: 308ms\tremaining: 19s\n",
      "4:\tlearn: 0.4013645\ttotal: 412ms\tremaining: 20.2s\n",
      "5:\tlearn: 0.3742588\ttotal: 522ms\tremaining: 21.2s\n",
      "6:\tlearn: 0.3604675\ttotal: 572ms\tremaining: 19.9s\n",
      "7:\tlearn: 0.3420366\ttotal: 695ms\tremaining: 21s\n",
      "8:\tlearn: 0.3283501\ttotal: 815ms\tremaining: 21.8s\n",
      "9:\tlearn: 0.3170553\ttotal: 936ms\tremaining: 22.5s\n",
      "10:\tlearn: 0.3084364\ttotal: 1.04s\tremaining: 22.6s\n",
      "11:\tlearn: 0.3049144\ttotal: 1.07s\tremaining: 21.2s\n",
      "12:\tlearn: 0.2983574\ttotal: 1.19s\tremaining: 21.6s\n",
      "13:\tlearn: 0.2920301\ttotal: 1.31s\tremaining: 22.1s\n",
      "14:\tlearn: 0.2878740\ttotal: 1.42s\tremaining: 22.2s\n",
      "15:\tlearn: 0.2854811\ttotal: 1.46s\tremaining: 21.3s\n",
      "16:\tlearn: 0.2818350\ttotal: 1.57s\tremaining: 21.6s\n",
      "17:\tlearn: 0.2782348\ttotal: 1.69s\tremaining: 21.8s\n",
      "18:\tlearn: 0.2755050\ttotal: 1.8s\tremaining: 21.9s\n",
      "19:\tlearn: 0.2731668\ttotal: 1.92s\tremaining: 22.1s\n",
      "20:\tlearn: 0.2692655\ttotal: 2.04s\tremaining: 22.2s\n",
      "21:\tlearn: 0.2676402\ttotal: 2.16s\tremaining: 22.4s\n",
      "22:\tlearn: 0.2650053\ttotal: 2.29s\tremaining: 22.6s\n",
      "23:\tlearn: 0.2626815\ttotal: 2.42s\tremaining: 22.8s\n",
      "24:\tlearn: 0.2608828\ttotal: 2.55s\tremaining: 22.9s\n",
      "25:\tlearn: 0.2595508\ttotal: 2.67s\tremaining: 23s\n",
      "26:\tlearn: 0.2580452\ttotal: 2.79s\tremaining: 23s\n",
      "27:\tlearn: 0.2570104\ttotal: 2.9s\tremaining: 23s\n",
      "28:\tlearn: 0.2558126\ttotal: 3.03s\tremaining: 23.1s\n",
      "29:\tlearn: 0.2548197\ttotal: 3.16s\tremaining: 23.2s\n",
      "30:\tlearn: 0.2536521\ttotal: 3.29s\tremaining: 23.3s\n",
      "31:\tlearn: 0.2529609\ttotal: 3.42s\tremaining: 23.3s\n",
      "32:\tlearn: 0.2519708\ttotal: 3.53s\tremaining: 23.2s\n",
      "33:\tlearn: 0.2508501\ttotal: 3.66s\tremaining: 23.2s\n",
      "34:\tlearn: 0.2499928\ttotal: 3.79s\tremaining: 23.3s\n",
      "35:\tlearn: 0.2490944\ttotal: 3.92s\tremaining: 23.3s\n",
      "36:\tlearn: 0.2485022\ttotal: 4.04s\tremaining: 23.3s\n",
      "37:\tlearn: 0.2477840\ttotal: 4.18s\tremaining: 23.4s\n",
      "38:\tlearn: 0.2472598\ttotal: 4.31s\tremaining: 23.3s\n",
      "39:\tlearn: 0.2466989\ttotal: 4.44s\tremaining: 23.3s\n",
      "40:\tlearn: 0.2463615\ttotal: 4.56s\tremaining: 23.2s\n",
      "41:\tlearn: 0.2456448\ttotal: 4.7s\tremaining: 23.3s\n",
      "42:\tlearn: 0.2446888\ttotal: 4.83s\tremaining: 23.3s\n",
      "43:\tlearn: 0.2441380\ttotal: 4.96s\tremaining: 23.2s\n",
      "44:\tlearn: 0.2438489\ttotal: 5.06s\tremaining: 23.1s\n",
      "45:\tlearn: 0.2437507\ttotal: 5.09s\tremaining: 22.6s\n",
      "46:\tlearn: 0.2433028\ttotal: 5.22s\tremaining: 22.5s\n",
      "47:\tlearn: 0.2423756\ttotal: 5.34s\tremaining: 22.5s\n",
      "48:\tlearn: 0.2417177\ttotal: 5.47s\tremaining: 22.5s\n",
      "49:\tlearn: 0.2411696\ttotal: 5.6s\tremaining: 22.4s\n",
      "50:\tlearn: 0.2407303\ttotal: 5.71s\tremaining: 22.3s\n",
      "51:\tlearn: 0.2399170\ttotal: 5.83s\tremaining: 22.2s\n",
      "52:\tlearn: 0.2394891\ttotal: 5.94s\tremaining: 22.1s\n",
      "53:\tlearn: 0.2390714\ttotal: 6.06s\tremaining: 22s\n",
      "54:\tlearn: 0.2386057\ttotal: 6.19s\tremaining: 21.9s\n",
      "55:\tlearn: 0.2382502\ttotal: 6.3s\tremaining: 21.8s\n",
      "56:\tlearn: 0.2374729\ttotal: 6.44s\tremaining: 21.8s\n",
      "57:\tlearn: 0.2368876\ttotal: 6.58s\tremaining: 21.8s\n",
      "58:\tlearn: 0.2364468\ttotal: 6.72s\tremaining: 21.8s\n",
      "59:\tlearn: 0.2360618\ttotal: 6.85s\tremaining: 21.7s\n",
      "60:\tlearn: 0.2357144\ttotal: 6.96s\tremaining: 21.6s\n",
      "61:\tlearn: 0.2354260\ttotal: 7.08s\tremaining: 21.5s\n",
      "62:\tlearn: 0.2350159\ttotal: 7.2s\tremaining: 21.4s\n",
      "63:\tlearn: 0.2346105\ttotal: 7.33s\tremaining: 21.3s\n",
      "64:\tlearn: 0.2340137\ttotal: 7.45s\tremaining: 21.2s\n",
      "65:\tlearn: 0.2336367\ttotal: 7.57s\tremaining: 21.1s\n",
      "66:\tlearn: 0.2332331\ttotal: 7.68s\tremaining: 21s\n",
      "67:\tlearn: 0.2325587\ttotal: 7.81s\tremaining: 20.9s\n",
      "68:\tlearn: 0.2320220\ttotal: 7.94s\tremaining: 20.8s\n",
      "69:\tlearn: 0.2315514\ttotal: 8.08s\tremaining: 20.8s\n",
      "70:\tlearn: 0.2309017\ttotal: 8.19s\tremaining: 20.7s\n",
      "71:\tlearn: 0.2303065\ttotal: 8.32s\tremaining: 20.6s\n",
      "72:\tlearn: 0.2301890\ttotal: 8.42s\tremaining: 20.4s\n",
      "73:\tlearn: 0.2299101\ttotal: 8.53s\tremaining: 20.3s\n",
      "74:\tlearn: 0.2297408\ttotal: 8.62s\tremaining: 20.1s\n",
      "75:\tlearn: 0.2291771\ttotal: 8.75s\tremaining: 20s\n",
      "76:\tlearn: 0.2289599\ttotal: 8.87s\tremaining: 19.9s\n",
      "77:\tlearn: 0.2289496\ttotal: 8.89s\tremaining: 19.6s\n",
      "78:\tlearn: 0.2284743\ttotal: 9.03s\tremaining: 19.5s\n",
      "79:\tlearn: 0.2283644\ttotal: 9.14s\tremaining: 19.4s\n",
      "80:\tlearn: 0.2279404\ttotal: 9.25s\tremaining: 19.3s\n",
      "81:\tlearn: 0.2273193\ttotal: 9.37s\tremaining: 19.2s\n",
      "82:\tlearn: 0.2269670\ttotal: 9.5s\tremaining: 19.1s\n",
      "83:\tlearn: 0.2267078\ttotal: 9.62s\tremaining: 19s\n",
      "84:\tlearn: 0.2260564\ttotal: 9.75s\tremaining: 18.9s\n",
      "85:\tlearn: 0.2258771\ttotal: 9.86s\tremaining: 18.8s\n",
      "86:\tlearn: 0.2256365\ttotal: 9.98s\tremaining: 18.7s\n",
      "87:\tlearn: 0.2253047\ttotal: 10.1s\tremaining: 18.6s\n",
      "88:\tlearn: 0.2251405\ttotal: 10.2s\tremaining: 18.5s\n",
      "89:\tlearn: 0.2246913\ttotal: 10.4s\tremaining: 18.4s\n",
      "90:\tlearn: 0.2246699\ttotal: 10.4s\tremaining: 18.1s\n",
      "91:\tlearn: 0.2243146\ttotal: 10.5s\tremaining: 18s\n",
      "92:\tlearn: 0.2239967\ttotal: 10.6s\tremaining: 17.9s\n",
      "93:\tlearn: 0.2236199\ttotal: 10.7s\tremaining: 17.8s\n",
      "94:\tlearn: 0.2234746\ttotal: 10.8s\tremaining: 17.7s\n",
      "95:\tlearn: 0.2231430\ttotal: 11s\tremaining: 17.6s\n",
      "96:\tlearn: 0.2227798\ttotal: 11.1s\tremaining: 17.5s\n",
      "97:\tlearn: 0.2226472\ttotal: 11.2s\tremaining: 17.3s\n",
      "98:\tlearn: 0.2224472\ttotal: 11.3s\tremaining: 17.2s\n",
      "99:\tlearn: 0.2219516\ttotal: 11.4s\tremaining: 17.1s\n",
      "100:\tlearn: 0.2217159\ttotal: 11.5s\tremaining: 17s\n",
      "101:\tlearn: 0.2212769\ttotal: 11.6s\tremaining: 16.9s\n",
      "102:\tlearn: 0.2209645\ttotal: 11.8s\tremaining: 16.8s\n",
      "103:\tlearn: 0.2205626\ttotal: 11.9s\tremaining: 16.7s\n",
      "104:\tlearn: 0.2200820\ttotal: 12s\tremaining: 16.6s\n",
      "105:\tlearn: 0.2198896\ttotal: 12.1s\tremaining: 16.5s\n",
      "106:\tlearn: 0.2194167\ttotal: 12.2s\tremaining: 16.4s\n",
      "107:\tlearn: 0.2189881\ttotal: 12.4s\tremaining: 16.3s\n",
      "108:\tlearn: 0.2188101\ttotal: 12.5s\tremaining: 16.1s\n",
      "109:\tlearn: 0.2184326\ttotal: 12.6s\tremaining: 16s\n",
      "110:\tlearn: 0.2179927\ttotal: 12.7s\tremaining: 15.9s\n",
      "111:\tlearn: 0.2174252\ttotal: 12.8s\tremaining: 15.8s\n",
      "112:\tlearn: 0.2172437\ttotal: 13s\tremaining: 15.7s\n",
      "113:\tlearn: 0.2170336\ttotal: 13.1s\tremaining: 15.6s\n",
      "114:\tlearn: 0.2168676\ttotal: 13.2s\tremaining: 15.5s\n",
      "115:\tlearn: 0.2166119\ttotal: 13.3s\tremaining: 15.4s\n",
      "116:\tlearn: 0.2162474\ttotal: 13.4s\tremaining: 15.3s\n",
      "117:\tlearn: 0.2158622\ttotal: 13.6s\tremaining: 15.2s\n",
      "118:\tlearn: 0.2157078\ttotal: 13.7s\tremaining: 15.1s\n",
      "119:\tlearn: 0.2152491\ttotal: 13.8s\tremaining: 15s\n",
      "120:\tlearn: 0.2148627\ttotal: 13.9s\tremaining: 14.8s\n",
      "121:\tlearn: 0.2145198\ttotal: 14s\tremaining: 14.7s\n",
      "122:\tlearn: 0.2141332\ttotal: 14.2s\tremaining: 14.6s\n",
      "123:\tlearn: 0.2138740\ttotal: 14.3s\tremaining: 14.5s\n",
      "124:\tlearn: 0.2132993\ttotal: 14.4s\tremaining: 14.4s\n",
      "125:\tlearn: 0.2129076\ttotal: 14.6s\tremaining: 14.3s\n",
      "126:\tlearn: 0.2126019\ttotal: 14.7s\tremaining: 14.2s\n",
      "127:\tlearn: 0.2124694\ttotal: 14.8s\tremaining: 14.1s\n",
      "128:\tlearn: 0.2122760\ttotal: 14.9s\tremaining: 14s\n",
      "129:\tlearn: 0.2118898\ttotal: 15s\tremaining: 13.9s\n",
      "130:\tlearn: 0.2115722\ttotal: 15.2s\tremaining: 13.8s\n",
      "131:\tlearn: 0.2112300\ttotal: 15.3s\tremaining: 13.7s\n",
      "132:\tlearn: 0.2108974\ttotal: 15.4s\tremaining: 13.6s\n",
      "133:\tlearn: 0.2104790\ttotal: 15.6s\tremaining: 13.5s\n",
      "134:\tlearn: 0.2102531\ttotal: 15.7s\tremaining: 13.4s\n",
      "135:\tlearn: 0.2099774\ttotal: 15.8s\tremaining: 13.2s\n",
      "136:\tlearn: 0.2097823\ttotal: 15.9s\tremaining: 13.1s\n",
      "137:\tlearn: 0.2095468\ttotal: 16.1s\tremaining: 13s\n",
      "138:\tlearn: 0.2092323\ttotal: 16.2s\tremaining: 12.9s\n",
      "139:\tlearn: 0.2090216\ttotal: 16.3s\tremaining: 12.8s\n",
      "140:\tlearn: 0.2088442\ttotal: 16.5s\tremaining: 12.7s\n",
      "141:\tlearn: 0.2087627\ttotal: 16.6s\tremaining: 12.6s\n",
      "142:\tlearn: 0.2083522\ttotal: 16.7s\tremaining: 12.5s\n",
      "143:\tlearn: 0.2079275\ttotal: 16.8s\tremaining: 12.4s\n",
      "144:\tlearn: 0.2076571\ttotal: 16.9s\tremaining: 12.3s\n",
      "145:\tlearn: 0.2073163\ttotal: 17.1s\tremaining: 12.2s\n",
      "146:\tlearn: 0.2070019\ttotal: 17.2s\tremaining: 12s\n",
      "147:\tlearn: 0.2066407\ttotal: 17.3s\tremaining: 11.9s\n",
      "148:\tlearn: 0.2062904\ttotal: 17.4s\tremaining: 11.8s\n",
      "149:\tlearn: 0.2061664\ttotal: 17.6s\tremaining: 11.7s\n",
      "150:\tlearn: 0.2058497\ttotal: 17.7s\tremaining: 11.6s\n",
      "151:\tlearn: 0.2056534\ttotal: 17.8s\tremaining: 11.5s\n",
      "152:\tlearn: 0.2055222\ttotal: 17.9s\tremaining: 11.4s\n",
      "153:\tlearn: 0.2051502\ttotal: 18.1s\tremaining: 11.3s\n",
      "154:\tlearn: 0.2048253\ttotal: 18.2s\tremaining: 11.1s\n",
      "155:\tlearn: 0.2046803\ttotal: 18.3s\tremaining: 11s\n",
      "156:\tlearn: 0.2043507\ttotal: 18.4s\tremaining: 10.9s\n",
      "157:\tlearn: 0.2039811\ttotal: 18.5s\tremaining: 10.8s\n",
      "158:\tlearn: 0.2037574\ttotal: 18.7s\tremaining: 10.7s\n",
      "159:\tlearn: 0.2035405\ttotal: 18.8s\tremaining: 10.6s\n",
      "160:\tlearn: 0.2033707\ttotal: 18.9s\tremaining: 10.4s\n",
      "161:\tlearn: 0.2029615\ttotal: 19s\tremaining: 10.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162:\tlearn: 0.2025790\ttotal: 19.1s\tremaining: 10.2s\n",
      "163:\tlearn: 0.2024872\ttotal: 19.3s\tremaining: 10.1s\n",
      "164:\tlearn: 0.2022924\ttotal: 19.4s\tremaining: 9.99s\n",
      "165:\tlearn: 0.2020293\ttotal: 19.5s\tremaining: 9.88s\n",
      "166:\tlearn: 0.2017983\ttotal: 19.6s\tremaining: 9.76s\n",
      "167:\tlearn: 0.2014788\ttotal: 19.8s\tremaining: 9.65s\n",
      "168:\tlearn: 0.2010933\ttotal: 19.9s\tremaining: 9.53s\n",
      "169:\tlearn: 0.2008900\ttotal: 20s\tremaining: 9.41s\n",
      "170:\tlearn: 0.2008412\ttotal: 20.1s\tremaining: 9.29s\n",
      "171:\tlearn: 0.2006006\ttotal: 20.2s\tremaining: 9.18s\n",
      "172:\tlearn: 0.2004064\ttotal: 20.4s\tremaining: 9.06s\n",
      "173:\tlearn: 0.2002010\ttotal: 20.5s\tremaining: 8.95s\n",
      "174:\tlearn: 0.1998810\ttotal: 20.6s\tremaining: 8.84s\n",
      "175:\tlearn: 0.1995788\ttotal: 20.8s\tremaining: 8.73s\n",
      "176:\tlearn: 0.1994259\ttotal: 20.9s\tremaining: 8.62s\n",
      "177:\tlearn: 0.1992435\ttotal: 21s\tremaining: 8.5s\n",
      "178:\tlearn: 0.1990360\ttotal: 21.1s\tremaining: 8.38s\n",
      "179:\tlearn: 0.1987461\ttotal: 21.3s\tremaining: 8.26s\n",
      "180:\tlearn: 0.1985351\ttotal: 21.4s\tremaining: 8.15s\n",
      "181:\tlearn: 0.1983532\ttotal: 21.5s\tremaining: 8.03s\n",
      "182:\tlearn: 0.1981707\ttotal: 21.6s\tremaining: 7.91s\n",
      "183:\tlearn: 0.1979375\ttotal: 21.7s\tremaining: 7.79s\n",
      "184:\tlearn: 0.1977490\ttotal: 21.9s\tremaining: 7.68s\n",
      "185:\tlearn: 0.1973670\ttotal: 22s\tremaining: 7.56s\n",
      "186:\tlearn: 0.1971993\ttotal: 22.1s\tremaining: 7.45s\n",
      "187:\tlearn: 0.1970227\ttotal: 22.2s\tremaining: 7.34s\n",
      "188:\tlearn: 0.1967430\ttotal: 22.4s\tremaining: 7.22s\n",
      "189:\tlearn: 0.1965658\ttotal: 22.5s\tremaining: 7.1s\n",
      "190:\tlearn: 0.1961086\ttotal: 22.6s\tremaining: 6.99s\n",
      "191:\tlearn: 0.1958319\ttotal: 22.8s\tremaining: 6.87s\n",
      "192:\tlearn: 0.1956168\ttotal: 22.9s\tremaining: 6.75s\n",
      "193:\tlearn: 0.1953488\ttotal: 23s\tremaining: 6.64s\n",
      "194:\tlearn: 0.1952861\ttotal: 23.1s\tremaining: 6.52s\n",
      "195:\tlearn: 0.1951482\ttotal: 23.2s\tremaining: 6.4s\n",
      "196:\tlearn: 0.1949343\ttotal: 23.4s\tremaining: 6.28s\n",
      "197:\tlearn: 0.1946419\ttotal: 23.5s\tremaining: 6.17s\n",
      "198:\tlearn: 0.1944659\ttotal: 23.6s\tremaining: 6.05s\n",
      "199:\tlearn: 0.1943210\ttotal: 23.7s\tremaining: 5.93s\n",
      "200:\tlearn: 0.1941327\ttotal: 23.8s\tremaining: 5.81s\n",
      "201:\tlearn: 0.1939022\ttotal: 24s\tremaining: 5.69s\n",
      "202:\tlearn: 0.1936938\ttotal: 24.1s\tremaining: 5.57s\n",
      "203:\tlearn: 0.1935355\ttotal: 24.2s\tremaining: 5.46s\n",
      "204:\tlearn: 0.1931937\ttotal: 24.3s\tremaining: 5.34s\n",
      "205:\tlearn: 0.1930420\ttotal: 24.5s\tremaining: 5.22s\n",
      "206:\tlearn: 0.1928762\ttotal: 24.6s\tremaining: 5.1s\n",
      "207:\tlearn: 0.1924568\ttotal: 24.7s\tremaining: 4.98s\n",
      "208:\tlearn: 0.1921602\ttotal: 24.8s\tremaining: 4.87s\n",
      "209:\tlearn: 0.1920251\ttotal: 24.9s\tremaining: 4.75s\n",
      "210:\tlearn: 0.1918884\ttotal: 25s\tremaining: 4.63s\n",
      "211:\tlearn: 0.1916886\ttotal: 25.1s\tremaining: 4.51s\n",
      "212:\tlearn: 0.1915545\ttotal: 25.3s\tremaining: 4.39s\n",
      "213:\tlearn: 0.1914098\ttotal: 25.4s\tremaining: 4.27s\n",
      "214:\tlearn: 0.1912046\ttotal: 25.5s\tremaining: 4.15s\n",
      "215:\tlearn: 0.1910052\ttotal: 25.6s\tremaining: 4.04s\n",
      "216:\tlearn: 0.1909381\ttotal: 25.7s\tremaining: 3.92s\n",
      "217:\tlearn: 0.1908262\ttotal: 25.9s\tremaining: 3.8s\n",
      "218:\tlearn: 0.1905919\ttotal: 26s\tremaining: 3.68s\n",
      "219:\tlearn: 0.1903487\ttotal: 26.1s\tremaining: 3.56s\n",
      "220:\tlearn: 0.1900237\ttotal: 26.2s\tremaining: 3.44s\n",
      "221:\tlearn: 0.1897514\ttotal: 26.4s\tremaining: 3.33s\n",
      "222:\tlearn: 0.1897225\ttotal: 26.5s\tremaining: 3.2s\n",
      "223:\tlearn: 0.1894358\ttotal: 26.6s\tremaining: 3.09s\n",
      "224:\tlearn: 0.1893362\ttotal: 26.7s\tremaining: 2.97s\n",
      "225:\tlearn: 0.1891315\ttotal: 26.8s\tremaining: 2.85s\n",
      "226:\tlearn: 0.1889472\ttotal: 27s\tremaining: 2.73s\n",
      "227:\tlearn: 0.1887593\ttotal: 27.1s\tremaining: 2.61s\n",
      "228:\tlearn: 0.1887442\ttotal: 27.2s\tremaining: 2.49s\n",
      "229:\tlearn: 0.1886162\ttotal: 27.3s\tremaining: 2.37s\n",
      "230:\tlearn: 0.1883048\ttotal: 27.4s\tremaining: 2.25s\n",
      "231:\tlearn: 0.1882408\ttotal: 27.5s\tremaining: 2.13s\n",
      "232:\tlearn: 0.1880325\ttotal: 27.6s\tremaining: 2.02s\n",
      "233:\tlearn: 0.1878393\ttotal: 27.8s\tremaining: 1.9s\n",
      "234:\tlearn: 0.1876924\ttotal: 27.9s\tremaining: 1.78s\n",
      "235:\tlearn: 0.1875683\ttotal: 28s\tremaining: 1.66s\n",
      "236:\tlearn: 0.1874262\ttotal: 28.1s\tremaining: 1.54s\n",
      "237:\tlearn: 0.1871671\ttotal: 28.3s\tremaining: 1.42s\n",
      "238:\tlearn: 0.1870624\ttotal: 28.4s\tremaining: 1.3s\n",
      "239:\tlearn: 0.1869479\ttotal: 28.5s\tremaining: 1.19s\n",
      "240:\tlearn: 0.1869225\ttotal: 28.6s\tremaining: 1.07s\n",
      "241:\tlearn: 0.1868653\ttotal: 28.7s\tremaining: 948ms\n",
      "242:\tlearn: 0.1867746\ttotal: 28.8s\tremaining: 830ms\n",
      "243:\tlearn: 0.1865140\ttotal: 28.9s\tremaining: 712ms\n",
      "244:\tlearn: 0.1863566\ttotal: 29.1s\tremaining: 593ms\n",
      "245:\tlearn: 0.1862387\ttotal: 29.2s\tremaining: 474ms\n",
      "246:\tlearn: 0.1861009\ttotal: 29.3s\tremaining: 356ms\n",
      "247:\tlearn: 0.1858963\ttotal: 29.4s\tremaining: 237ms\n",
      "248:\tlearn: 0.1858355\ttotal: 29.5s\tremaining: 119ms\n",
      "249:\tlearn: 0.1855490\ttotal: 29.7s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "model.fit(raw_df.drop(\"defective\",axis=1), raw_df.defective,cat_features=cat_features)\n",
    "p3 = model.predict_proba(test)[:,1]\n",
    "ct2_ts = [1 if i > 0.4 else 0 for i in p3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = [1 if i == \"yes\" else 0 for i in rf_ts]\n",
    "c = [1 if i>=2 else 0 for i in (np.array(r) + np.array(ct_ts) + np.array(ct2_ts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1591"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ((np.array(r) + np.array(ct_ts) + np.array(ct2_ts)) == 1).sum()\n",
    "# (lab_act.defective == \"no\").sum()\n",
    "(np.array(c) == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4936"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(c) == np.array(ct2_ts)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"id\",\"defective\"])\n",
    "df.id = afr_test.new_ids\n",
    "df.defective = ct2_ts\n",
    "df.to_csv(\"F02691_F02155.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>defective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4168</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24074</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19227</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22751</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8556</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>48413</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5853</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>36654</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>40398</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>49235</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>26676</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>32770</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>58318</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16559</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>12752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>49720</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1650</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>54359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>38776</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>40094</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>32680</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>45490</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>27470</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>11918</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15619</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13395</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>46706</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>884</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5053</th>\n",
       "      <td>15882</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5054</th>\n",
       "      <td>377</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5055</th>\n",
       "      <td>32869</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5056</th>\n",
       "      <td>19807</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5057</th>\n",
       "      <td>27336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5058</th>\n",
       "      <td>33341</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5059</th>\n",
       "      <td>46684</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5060</th>\n",
       "      <td>56817</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5061</th>\n",
       "      <td>3221</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5062</th>\n",
       "      <td>20588</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5063</th>\n",
       "      <td>36173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5064</th>\n",
       "      <td>44304</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5065</th>\n",
       "      <td>29520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5066</th>\n",
       "      <td>38349</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5067</th>\n",
       "      <td>33406</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5068</th>\n",
       "      <td>11155</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5069</th>\n",
       "      <td>49246</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5070</th>\n",
       "      <td>5414</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5071</th>\n",
       "      <td>14859</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5072</th>\n",
       "      <td>7489</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5073</th>\n",
       "      <td>5658</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5074</th>\n",
       "      <td>14776</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5075</th>\n",
       "      <td>11999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5076</th>\n",
       "      <td>51052</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5077</th>\n",
       "      <td>25126</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5078</th>\n",
       "      <td>38463</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5079</th>\n",
       "      <td>36374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5080</th>\n",
       "      <td>5214</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5081</th>\n",
       "      <td>24248</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5082</th>\n",
       "      <td>56165</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5083 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  defective\n",
       "0      4168          1\n",
       "1     24074          1\n",
       "2     19227          1\n",
       "3     44693          0\n",
       "4     22751          1\n",
       "5     51896          1\n",
       "6      8556          1\n",
       "7     48413          1\n",
       "8      5853          0\n",
       "9     36654          1\n",
       "10    40398          1\n",
       "11    49235          1\n",
       "12    26676          1\n",
       "13    32770          0\n",
       "14    58318          1\n",
       "15    16559          1\n",
       "16    12752          0\n",
       "17    49720          1\n",
       "18     1650          1\n",
       "19    54359          1\n",
       "20    38776          0\n",
       "21    40094          1\n",
       "22    32680          0\n",
       "23    45490          1\n",
       "24    27470          0\n",
       "25    11918          0\n",
       "26    15619          1\n",
       "27    13395          0\n",
       "28    46706          1\n",
       "29      884          1\n",
       "...     ...        ...\n",
       "5053  15882          1\n",
       "5054    377          0\n",
       "5055  32869          0\n",
       "5056  19807          0\n",
       "5057  27336          1\n",
       "5058  33341          1\n",
       "5059  46684          0\n",
       "5060  56817          0\n",
       "5061   3221          0\n",
       "5062  20588          0\n",
       "5063  36173          0\n",
       "5064  44304          0\n",
       "5065  29520          1\n",
       "5066  38349          1\n",
       "5067  33406          1\n",
       "5068  11155          1\n",
       "5069  49246          0\n",
       "5070   5414          0\n",
       "5071  14859          1\n",
       "5072   7489          1\n",
       "5073   5658          0\n",
       "5074  14776          1\n",
       "5075  11999          0\n",
       "5076  51052          1\n",
       "5077  25126          1\n",
       "5078  38463          1\n",
       "5079  36374          0\n",
       "5080   5214          1\n",
       "5081  24248          1\n",
       "5082  56165          0\n",
       "\n",
       "[5083 rows x 2 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.read_csv(\"F02691_F02155.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
